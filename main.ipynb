{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nodes: 5201\n",
      "num of edges: 396846\n",
      "dimension of feature: (5201, 2089)\n",
      "class of labels: 5\n",
      "train Examples: 2496\n",
      "val Examples: 1664\n",
      "test Examples: 1041\n"
     ]
    }
   ],
   "source": [
    "from load_hetero_data import HeteroDataset\n",
    "# Name of dataset\n",
    "name=\"squirrel\"\n",
    "# Load dataset with fixed split\n",
    "dataset=HeteroDataset(name,\"./dataset/splits/\"+name+\"_split_0.npz\")\n",
    "print(\"num of nodes:\",dataset.graph.num_nodes)\n",
    "print(\"num of edges:\",dataset.graph.num_edges)\n",
    "print(\"dimension of feature:\",dataset.graph.node_feat[\"feat\"].shape)\n",
    "print(\"class of labels:\", int(max(dataset.y))+1)\n",
    "print(\"train Examples:\",len(dataset.train_index))\n",
    "print(\"val Examples:\",len(dataset.val_index))\n",
    "print(\"test Examples:\",len(dataset.test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import pgl\n",
    "\n",
    "class GCN(nn.Layer):\n",
    "    \"\"\"Implement of GCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_class,\n",
    "                 num_layers=2,\n",
    "                 hidden_size=16,\n",
    "                 **kwargs):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gcns = nn.LayerList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                self.gcns.append(\n",
    "                    pgl.nn.GCNConv(\n",
    "                        input_size,\n",
    "                        self.hidden_size,\n",
    "                        activation=\"relu\",\n",
    "                        norm=True))\n",
    "            else:\n",
    "                self.gcns.append(\n",
    "                    pgl.nn.GCNConv(\n",
    "                        self.hidden_size,\n",
    "                        self.hidden_size,\n",
    "                        activation=\"relu\",\n",
    "                        norm=True))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_size, self.num_class)\n",
    "    def forward(self, graph, feature):\n",
    "        for m in self.gcns:\n",
    "            feature = m(graph, feature)\n",
    "        logits = self.output(feature)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "g = g.tensor()\n",
    "y = paddle.to_tensor(dataset.y)\n",
    "gcn = GCN(g.node_feat[\"feat\"].shape[1], 5)\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "optim = Adam(learning_rate=0.01,\n",
    "             parameters=gcn.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21229586935638808\n",
      "epoch: 0 | train_loss: 1.6095 | val_loss:1.6095 | test_loss: 1.6094\n",
      "0.2084534101825168\n",
      "epoch: 1 | train_loss: 1.6087 | val_loss:1.6095 | test_loss: 1.6090\n",
      "0.1930835734870317\n",
      "epoch: 2 | train_loss: 1.6070 | val_loss:1.6082 | test_loss: 1.6076\n",
      "0.19692603266090297\n",
      "epoch: 3 | train_loss: 1.6053 | val_loss:1.6065 | test_loss: 1.6061\n",
      "0.2334293948126801\n",
      "epoch: 4 | train_loss: 1.6035 | val_loss:1.6046 | test_loss: 1.6044\n",
      "0.2574447646493756\n",
      "epoch: 5 | train_loss: 1.6014 | val_loss:1.6026 | test_loss: 1.6023\n",
      "0.2641690682036503\n",
      "epoch: 6 | train_loss: 1.5990 | val_loss:1.6003 | test_loss: 1.6000\n",
      "0.2641690682036503\n",
      "epoch: 7 | train_loss: 1.5963 | val_loss:1.5979 | test_loss: 1.5974\n",
      "0.2737752161383285\n",
      "epoch: 8 | train_loss: 1.5934 | val_loss:1.5951 | test_loss: 1.5945\n",
      "0.27761767531219983\n",
      "epoch: 9 | train_loss: 1.5900 | val_loss:1.5919 | test_loss: 1.5913\n",
      "0.2804995196926033\n",
      "epoch: 10 | train_loss: 1.5863 | val_loss:1.5885 | test_loss: 1.5878\n",
      "0.2862632084534102\n",
      "epoch: 11 | train_loss: 1.5822 | val_loss:1.5849 | test_loss: 1.5838\n",
      "0.28914505283381364\n",
      "epoch: 12 | train_loss: 1.5777 | val_loss:1.5809 | test_loss: 1.5796\n",
      "0.2987512007684918\n",
      "epoch: 13 | train_loss: 1.5728 | val_loss:1.5766 | test_loss: 1.5750\n",
      "0.2987512007684918\n",
      "epoch: 14 | train_loss: 1.5674 | val_loss:1.5717 | test_loss: 1.5700\n",
      "0.297790585975024\n",
      "epoch: 15 | train_loss: 1.5618 | val_loss:1.5666 | test_loss: 1.5648\n",
      "0.297790585975024\n",
      "epoch: 16 | train_loss: 1.5557 | val_loss:1.5615 | test_loss: 1.5594\n",
      "0.29586935638808837\n",
      "epoch: 17 | train_loss: 1.5492 | val_loss:1.5560 | test_loss: 1.5536\n",
      "0.3016330451488953\n",
      "epoch: 18 | train_loss: 1.5422 | val_loss:1.5499 | test_loss: 1.5473\n",
      "0.3025936599423631\n",
      "epoch: 19 | train_loss: 1.5348 | val_loss:1.5436 | test_loss: 1.5406\n",
      "0.30931796349663787\n",
      "epoch: 20 | train_loss: 1.5268 | val_loss:1.5372 | test_loss: 1.5335\n",
      "0.31892411143131605\n",
      "epoch: 21 | train_loss: 1.5187 | val_loss:1.5309 | test_loss: 1.5263\n",
      "0.3227665706051873\n",
      "epoch: 22 | train_loss: 1.5102 | val_loss:1.5238 | test_loss: 1.5187\n",
      "0.3275696445725264\n",
      "epoch: 23 | train_loss: 1.5014 | val_loss:1.5168 | test_loss: 1.5110\n",
      "0.33525456292026895\n",
      "epoch: 24 | train_loss: 1.4925 | val_loss:1.5104 | test_loss: 1.5035\n",
      "0.34390009606147937\n",
      "epoch: 25 | train_loss: 1.4834 | val_loss:1.5035 | test_loss: 1.4957\n",
      "0.34870317002881845\n",
      "epoch: 26 | train_loss: 1.4743 | val_loss:1.4963 | test_loss: 1.4876\n",
      "0.3410182516810759\n",
      "epoch: 27 | train_loss: 1.4651 | val_loss:1.4901 | test_loss: 1.4798\n",
      "0.3467819404418828\n",
      "epoch: 28 | train_loss: 1.4558 | val_loss:1.4835 | test_loss: 1.4719\n",
      "0.34870317002881845\n",
      "epoch: 29 | train_loss: 1.4465 | val_loss:1.4766 | test_loss: 1.4639\n",
      "0.33429394812680113\n",
      "epoch: 30 | train_loss: 1.4372 | val_loss:1.4706 | test_loss: 1.4563\n",
      "0.3467819404418828\n",
      "epoch: 31 | train_loss: 1.4279 | val_loss:1.4641 | test_loss: 1.4484\n",
      "0.36983669548511044\n",
      "epoch: 32 | train_loss: 1.4188 | val_loss:1.4577 | test_loss: 1.4407\n",
      "0.37271853986551395\n",
      "epoch: 33 | train_loss: 1.4099 | val_loss:1.4525 | test_loss: 1.4338\n",
      "0.36407300672430354\n",
      "epoch: 34 | train_loss: 1.4013 | val_loss:1.4457 | test_loss: 1.4261\n",
      "0.37079731027857826\n",
      "epoch: 35 | train_loss: 1.3928 | val_loss:1.4422 | test_loss: 1.4207\n",
      "0.37175792507204614\n",
      "epoch: 36 | train_loss: 1.3844 | val_loss:1.4352 | test_loss: 1.4128\n",
      "0.3746397694524496\n",
      "epoch: 37 | train_loss: 1.3760 | val_loss:1.4318 | test_loss: 1.4073\n",
      "0.37367915465898177\n",
      "epoch: 38 | train_loss: 1.3679 | val_loss:1.4268 | test_loss: 1.4007\n",
      "0.3813640730067243\n",
      "epoch: 39 | train_loss: 1.3599 | val_loss:1.4217 | test_loss: 1.3942\n",
      "0.39961575408261285\n",
      "epoch: 40 | train_loss: 1.3522 | val_loss:1.4193 | test_loss: 1.3898\n",
      "0.38808837656099904\n",
      "epoch: 41 | train_loss: 1.3448 | val_loss:1.4126 | test_loss: 1.3823\n",
      "0.4092219020172911\n",
      "epoch: 42 | train_loss: 1.3372 | val_loss:1.4118 | test_loss: 1.3793\n",
      "0.3919308357348703\n",
      "epoch: 43 | train_loss: 1.3294 | val_loss:1.4051 | test_loss: 1.3717\n",
      "0.3900096061479347\n",
      "epoch: 44 | train_loss: 1.3219 | val_loss:1.4013 | test_loss: 1.3665\n",
      "0.4111431316042267\n",
      "epoch: 45 | train_loss: 1.3149 | val_loss:1.3997 | test_loss: 1.3632\n",
      "0.3919308357348703\n",
      "epoch: 46 | train_loss: 1.3080 | val_loss:1.3930 | test_loss: 1.3565\n",
      "0.4169068203650336\n",
      "epoch: 47 | train_loss: 1.3009 | val_loss:1.3930 | test_loss: 1.3544\n",
      "0.4092219020172911\n",
      "epoch: 48 | train_loss: 1.2937 | val_loss:1.3866 | test_loss: 1.3480\n",
      "0.4140249759846302\n",
      "epoch: 49 | train_loss: 1.2870 | val_loss:1.3832 | test_loss: 1.3442\n",
      "0.41786743515850144\n",
      "epoch: 50 | train_loss: 1.2808 | val_loss:1.3828 | test_loss: 1.3424\n",
      "0.4092219020172911\n",
      "epoch: 51 | train_loss: 1.2748 | val_loss:1.3763 | test_loss: 1.3370\n",
      "0.42459173871277617\n",
      "epoch: 52 | train_loss: 1.2682 | val_loss:1.3775 | test_loss: 1.3356\n",
      "0.41210374639769454\n",
      "epoch: 53 | train_loss: 1.2614 | val_loss:1.3716 | test_loss: 1.3304\n",
      "0.414985590778098\n",
      "epoch: 54 | train_loss: 1.2552 | val_loss:1.3692 | test_loss: 1.3276\n",
      "0.42267050912584053\n",
      "epoch: 55 | train_loss: 1.2494 | val_loss:1.3695 | test_loss: 1.3263\n",
      "0.4217098943323727\n",
      "epoch: 56 | train_loss: 1.2437 | val_loss:1.3633 | test_loss: 1.3215\n",
      "0.4303554274735831\n",
      "epoch: 57 | train_loss: 1.2374 | val_loss:1.3649 | test_loss: 1.3207\n",
      "0.4197886647454371\n",
      "epoch: 58 | train_loss: 1.2307 | val_loss:1.3587 | test_loss: 1.3159\n",
      "0.4274735830931796\n",
      "epoch: 59 | train_loss: 1.2245 | val_loss:1.3568 | test_loss: 1.3133\n",
      "0.43419788664745435\n",
      "epoch: 60 | train_loss: 1.2187 | val_loss:1.3565 | test_loss: 1.3116\n",
      "0.4303554274735831\n",
      "epoch: 61 | train_loss: 1.2130 | val_loss:1.3512 | test_loss: 1.3074\n",
      "0.4409221902017291\n",
      "epoch: 62 | train_loss: 1.2071 | val_loss:1.3540 | test_loss: 1.3076\n",
      "0.42843419788664744\n",
      "epoch: 63 | train_loss: 1.2010 | val_loss:1.3466 | test_loss: 1.3021\n",
      "0.4409221902017291\n",
      "epoch: 64 | train_loss: 1.1945 | val_loss:1.3484 | test_loss: 1.3011\n",
      "0.4322766570605187\n",
      "epoch: 65 | train_loss: 1.1881 | val_loss:1.3434 | test_loss: 1.2968\n",
      "0.4399615754082613\n",
      "epoch: 66 | train_loss: 1.1821 | val_loss:1.3418 | test_loss: 1.2948\n",
      "0.4399615754082613\n",
      "epoch: 67 | train_loss: 1.1764 | val_loss:1.3425 | test_loss: 1.2938\n",
      "0.4457252641690682\n",
      "epoch: 68 | train_loss: 1.1709 | val_loss:1.3375 | test_loss: 1.2900\n",
      "0.43707973102785785\n",
      "epoch: 69 | train_loss: 1.1656 | val_loss:1.3427 | test_loss: 1.2923\n",
      "0.4543707973102786\n",
      "epoch: 70 | train_loss: 1.1603 | val_loss:1.3347 | test_loss: 1.2871\n",
      "0.44188280499519694\n",
      "epoch: 71 | train_loss: 1.1543 | val_loss:1.3409 | test_loss: 1.2891\n",
      "0.44860710854947167\n",
      "epoch: 72 | train_loss: 1.1475 | val_loss:1.3326 | test_loss: 1.2835\n",
      "0.4495677233429395\n",
      "epoch: 73 | train_loss: 1.1414 | val_loss:1.3332 | test_loss: 1.2830\n",
      "0.4514889529298751\n",
      "epoch: 74 | train_loss: 1.1363 | val_loss:1.3346 | test_loss: 1.2829\n",
      "0.45244956772334294\n",
      "epoch: 75 | train_loss: 1.1316 | val_loss:1.3294 | test_loss: 1.2797\n",
      "0.44668587896253603\n",
      "epoch: 76 | train_loss: 1.1265 | val_loss:1.3362 | test_loss: 1.2836\n",
      "0.4601344860710855\n",
      "epoch: 77 | train_loss: 1.1204 | val_loss:1.3279 | test_loss: 1.2778\n",
      "0.45917387127761766\n",
      "epoch: 78 | train_loss: 1.1143 | val_loss:1.3309 | test_loss: 1.2785\n",
      "0.46301633045148893\n",
      "epoch: 79 | train_loss: 1.1088 | val_loss:1.3294 | test_loss: 1.2778\n",
      "0.46301633045148893\n",
      "epoch: 80 | train_loss: 1.1040 | val_loss:1.3266 | test_loss: 1.2758\n",
      "0.46301633045148893\n",
      "epoch: 81 | train_loss: 1.0996 | val_loss:1.3332 | test_loss: 1.2793\n",
      "0.4610951008645533\n",
      "epoch: 82 | train_loss: 1.0947 | val_loss:1.3258 | test_loss: 1.2753\n",
      "0.45917387127761766\n",
      "epoch: 83 | train_loss: 1.0893 | val_loss:1.3330 | test_loss: 1.2789\n",
      "0.47166186359269935\n",
      "epoch: 84 | train_loss: 1.0834 | val_loss:1.3263 | test_loss: 1.2740\n",
      "0.4687800192122959\n",
      "epoch: 85 | train_loss: 1.0779 | val_loss:1.3280 | test_loss: 1.2752\n",
      "0.4620557156580211\n",
      "epoch: 86 | train_loss: 1.0731 | val_loss:1.3296 | test_loss: 1.2755\n",
      "0.4793467819404419\n",
      "epoch: 87 | train_loss: 1.0689 | val_loss:1.3256 | test_loss: 1.2733\n",
      "0.4610951008645533\n",
      "epoch: 88 | train_loss: 1.0648 | val_loss:1.3344 | test_loss: 1.2790\n",
      "0.47742555235350626\n",
      "epoch: 89 | train_loss: 1.0605 | val_loss:1.3259 | test_loss: 1.2734\n",
      "0.4658981748318924\n",
      "epoch: 90 | train_loss: 1.0554 | val_loss:1.3354 | test_loss: 1.2790\n",
      "0.473583093179635\n",
      "epoch: 91 | train_loss: 1.0495 | val_loss:1.3267 | test_loss: 1.2735\n",
      "0.4755043227665706\n",
      "epoch: 92 | train_loss: 1.0441 | val_loss:1.3298 | test_loss: 1.2743\n",
      "0.47070124879923153\n",
      "epoch: 93 | train_loss: 1.0396 | val_loss:1.3311 | test_loss: 1.2757\n",
      "0.47742555235350626\n",
      "epoch: 94 | train_loss: 1.0359 | val_loss:1.3275 | test_loss: 1.2734\n",
      "0.47166186359269935\n",
      "epoch: 95 | train_loss: 1.0323 | val_loss:1.3377 | test_loss: 1.2795\n",
      "0.4860710854947166\n",
      "epoch: 96 | train_loss: 1.0282 | val_loss:1.3288 | test_loss: 1.2748\n",
      "0.4793467819404419\n",
      "epoch: 97 | train_loss: 1.0233 | val_loss:1.3391 | test_loss: 1.2802\n",
      "0.4803073967339097\n",
      "epoch: 98 | train_loss: 1.0176 | val_loss:1.3307 | test_loss: 1.2747\n",
      "0.48895292987512007\n",
      "epoch: 99 | train_loss: 1.0127 | val_loss:1.3334 | test_loss: 1.2768\n",
      "0.4860710854947166\n",
      "epoch: 100 | train_loss: 1.0088 | val_loss:1.3366 | test_loss: 1.2776\n",
      "0.4908741594620557\n",
      "epoch: 101 | train_loss: 1.0054 | val_loss:1.3324 | test_loss: 1.2766\n",
      "0.4899135446685879\n",
      "epoch: 102 | train_loss: 1.0020 | val_loss:1.3437 | test_loss: 1.2832\n",
      "0.494716618635927\n",
      "epoch: 103 | train_loss: 0.9980 | val_loss:1.3348 | test_loss: 1.2770\n",
      "0.48895292987512007\n",
      "epoch: 104 | train_loss: 0.9937 | val_loss:1.3445 | test_loss: 1.2857\n",
      "0.5014409221902018\n",
      "epoch: 105 | train_loss: 0.9888 | val_loss:1.3372 | test_loss: 1.2772\n",
      "0.4908741594620557\n",
      "epoch: 106 | train_loss: 0.9839 | val_loss:1.3397 | test_loss: 1.2816\n",
      "0.4860710854947166\n",
      "epoch: 107 | train_loss: 0.9799 | val_loss:1.3420 | test_loss: 1.2824\n",
      "0.505283381364073\n",
      "epoch: 108 | train_loss: 0.9770 | val_loss:1.3399 | test_loss: 1.2798\n",
      "0.49279538904899134\n",
      "epoch: 109 | train_loss: 0.9739 | val_loss:1.3489 | test_loss: 1.2891\n",
      "0.5014409221902018\n",
      "epoch: 110 | train_loss: 0.9699 | val_loss:1.3406 | test_loss: 1.2815\n",
      "0.494716618635927\n",
      "epoch: 111 | train_loss: 0.9660 | val_loss:1.3501 | test_loss: 1.2875\n",
      "0.5014409221902018\n",
      "epoch: 112 | train_loss: 0.9623 | val_loss:1.3424 | test_loss: 1.2859\n",
      "0.4966378482228626\n",
      "epoch: 113 | train_loss: 0.9583 | val_loss:1.3506 | test_loss: 1.2869\n",
      "0.5014409221902018\n",
      "epoch: 114 | train_loss: 0.9537 | val_loss:1.3455 | test_loss: 1.2871\n",
      "0.5004803073967339\n",
      "epoch: 115 | train_loss: 0.9498 | val_loss:1.3484 | test_loss: 1.2888\n",
      "0.4966378482228626\n",
      "epoch: 116 | train_loss: 0.9466 | val_loss:1.3503 | test_loss: 1.2878\n",
      "0.5024015369836695\n",
      "epoch: 117 | train_loss: 0.9432 | val_loss:1.3482 | test_loss: 1.2908\n",
      "0.49951969260326606\n",
      "epoch: 118 | train_loss: 0.9395 | val_loss:1.3541 | test_loss: 1.2918\n",
      "0.505283381364073\n",
      "epoch: 119 | train_loss: 0.9363 | val_loss:1.3493 | test_loss: 1.2899\n",
      "0.49759846301633043\n",
      "epoch: 120 | train_loss: 0.9339 | val_loss:1.3599 | test_loss: 1.2998\n",
      "0.5110470701248799\n",
      "epoch: 121 | train_loss: 0.9318 | val_loss:1.3518 | test_loss: 1.2915\n",
      "0.49855907780979825\n",
      "epoch: 122 | train_loss: 0.9296 | val_loss:1.3675 | test_loss: 1.3056\n",
      "0.5072046109510087\n",
      "epoch: 123 | train_loss: 0.9268 | val_loss:1.3525 | test_loss: 1.2960\n",
      "0.505283381364073\n",
      "epoch: 124 | train_loss: 0.9236 | val_loss:1.3712 | test_loss: 1.3052\n",
      "0.505283381364073\n",
      "epoch: 125 | train_loss: 0.9175 | val_loss:1.3554 | test_loss: 1.2992\n",
      "0.505283381364073\n",
      "epoch: 126 | train_loss: 0.9122 | val_loss:1.3595 | test_loss: 1.2994\n",
      "0.505283381364073\n",
      "epoch: 127 | train_loss: 0.9106 | val_loss:1.3658 | test_loss: 1.3027\n",
      "0.5072046109510087\n",
      "epoch: 128 | train_loss: 0.9093 | val_loss:1.3572 | test_loss: 1.3016\n",
      "0.5024015369836695\n",
      "epoch: 129 | train_loss: 0.9061 | val_loss:1.3725 | test_loss: 1.3095\n",
      "0.5148895292987512\n",
      "epoch: 130 | train_loss: 0.9017 | val_loss:1.3617 | test_loss: 1.3014\n",
      "0.505283381364073\n",
      "epoch: 131 | train_loss: 0.8984 | val_loss:1.3670 | test_loss: 1.3100\n",
      "0.515850144092219\n",
      "epoch: 132 | train_loss: 0.8952 | val_loss:1.3696 | test_loss: 1.3059\n",
      "0.515850144092219\n",
      "epoch: 133 | train_loss: 0.8920 | val_loss:1.3635 | test_loss: 1.3050\n",
      "0.5004803073967339\n",
      "epoch: 134 | train_loss: 0.8902 | val_loss:1.3737 | test_loss: 1.3148\n",
      "0.5168107588856868\n",
      "epoch: 135 | train_loss: 0.8877 | val_loss:1.3695 | test_loss: 1.3066\n",
      "0.5062439961575408\n",
      "epoch: 136 | train_loss: 0.8835 | val_loss:1.3744 | test_loss: 1.3156\n",
      "0.5139289145052833\n",
      "epoch: 137 | train_loss: 0.8795 | val_loss:1.3718 | test_loss: 1.3112\n",
      "0.5177713736791547\n",
      "epoch: 138 | train_loss: 0.8771 | val_loss:1.3721 | test_loss: 1.3108\n",
      "0.5091258405379443\n",
      "epoch: 139 | train_loss: 0.8751 | val_loss:1.3785 | test_loss: 1.3195\n",
      "0.5196926032660903\n",
      "epoch: 140 | train_loss: 0.8721 | val_loss:1.3739 | test_loss: 1.3132\n",
      "0.5081652257444764\n",
      "epoch: 141 | train_loss: 0.8691 | val_loss:1.3836 | test_loss: 1.3211\n",
      "0.5187319884726225\n",
      "epoch: 142 | train_loss: 0.8662 | val_loss:1.3767 | test_loss: 1.3183\n",
      "0.515850144092219\n",
      "epoch: 143 | train_loss: 0.8643 | val_loss:1.3857 | test_loss: 1.3198\n",
      "0.5062439961575408\n",
      "epoch: 144 | train_loss: 0.8621 | val_loss:1.3804 | test_loss: 1.3252\n",
      "0.5196926032660903\n",
      "epoch: 145 | train_loss: 0.8585 | val_loss:1.3877 | test_loss: 1.3221\n",
      "0.5139289145052833\n",
      "epoch: 146 | train_loss: 0.8541 | val_loss:1.3845 | test_loss: 1.3247\n",
      "0.5139289145052833\n",
      "epoch: 147 | train_loss: 0.8520 | val_loss:1.3875 | test_loss: 1.3294\n",
      "0.515850144092219\n",
      "epoch: 148 | train_loss: 0.8505 | val_loss:1.3922 | test_loss: 1.3263\n",
      "0.5120076849183477\n",
      "epoch: 149 | train_loss: 0.8473 | val_loss:1.3889 | test_loss: 1.3323\n",
      "0.5072046109510087\n",
      "epoch: 150 | train_loss: 0.8436 | val_loss:1.3926 | test_loss: 1.3286\n",
      "0.5196926032660903\n",
      "epoch: 151 | train_loss: 0.8403 | val_loss:1.3907 | test_loss: 1.3303\n",
      "0.5129682997118156\n",
      "epoch: 152 | train_loss: 0.8383 | val_loss:1.3975 | test_loss: 1.3378\n",
      "0.5187319884726225\n",
      "epoch: 153 | train_loss: 0.8371 | val_loss:1.3971 | test_loss: 1.3326\n",
      "0.5120076849183477\n",
      "epoch: 154 | train_loss: 0.8355 | val_loss:1.4044 | test_loss: 1.3468\n",
      "0.5206532180595581\n",
      "epoch: 155 | train_loss: 0.8329 | val_loss:1.3978 | test_loss: 1.3343\n",
      "0.5024015369836695\n",
      "epoch: 156 | train_loss: 0.8298 | val_loss:1.4102 | test_loss: 1.3476\n",
      "0.5235350624399616\n",
      "epoch: 157 | train_loss: 0.8295 | val_loss:1.3999 | test_loss: 1.3443\n",
      "0.515850144092219\n",
      "epoch: 158 | train_loss: 0.8325 | val_loss:1.4296 | test_loss: 1.3575\n",
      "0.515850144092219\n",
      "epoch: 159 | train_loss: 0.8288 | val_loss:1.4057 | test_loss: 1.3527\n",
      "0.5091258405379443\n",
      "epoch: 160 | train_loss: 0.8215 | val_loss:1.4217 | test_loss: 1.3556\n",
      "0.5225744476464937\n",
      "epoch: 161 | train_loss: 0.8164 | val_loss:1.4077 | test_loss: 1.3450\n",
      "0.5235350624399616\n",
      "epoch: 162 | train_loss: 0.8159 | val_loss:1.4113 | test_loss: 1.3570\n",
      "0.515850144092219\n",
      "epoch: 163 | train_loss: 0.8141 | val_loss:1.4230 | test_loss: 1.3552\n",
      "0.5283381364073007\n",
      "epoch: 164 | train_loss: 0.8098 | val_loss:1.4112 | test_loss: 1.3526\n",
      "0.5148895292987512\n",
      "epoch: 165 | train_loss: 0.8098 | val_loss:1.4251 | test_loss: 1.3683\n",
      "0.5196926032660903\n",
      "epoch: 166 | train_loss: 0.8090 | val_loss:1.4222 | test_loss: 1.3555\n",
      "0.5235350624399616\n",
      "epoch: 167 | train_loss: 0.8023 | val_loss:1.4196 | test_loss: 1.3619\n",
      "0.5273775216138329\n",
      "epoch: 168 | train_loss: 0.8004 | val_loss:1.4215 | test_loss: 1.3645\n",
      "0.5225744476464937\n",
      "epoch: 169 | train_loss: 0.8014 | val_loss:1.4265 | test_loss: 1.3608\n",
      "0.5196926032660903\n",
      "epoch: 170 | train_loss: 0.7969 | val_loss:1.4304 | test_loss: 1.3728\n",
      "0.5292987512007685\n",
      "epoch: 171 | train_loss: 0.7939 | val_loss:1.4237 | test_loss: 1.3670\n",
      "0.5244956772334294\n",
      "epoch: 172 | train_loss: 0.7937 | val_loss:1.4348 | test_loss: 1.3688\n",
      "0.526416906820365\n",
      "epoch: 173 | train_loss: 0.7900 | val_loss:1.4289 | test_loss: 1.3736\n",
      "0.5360230547550432\n",
      "epoch: 174 | train_loss: 0.7866 | val_loss:1.4301 | test_loss: 1.3715\n",
      "0.5321805955811719\n",
      "epoch: 175 | train_loss: 0.7861 | val_loss:1.4386 | test_loss: 1.3747\n",
      "0.5273775216138329\n",
      "epoch: 176 | train_loss: 0.7844 | val_loss:1.4339 | test_loss: 1.3799\n",
      "0.526416906820365\n",
      "epoch: 177 | train_loss: 0.7811 | val_loss:1.4413 | test_loss: 1.3794\n",
      "0.5331412103746398\n",
      "epoch: 178 | train_loss: 0.7787 | val_loss:1.4371 | test_loss: 1.3773\n",
      "0.5273775216138329\n",
      "epoch: 179 | train_loss: 0.7776 | val_loss:1.4423 | test_loss: 1.3868\n",
      "0.5350624399615754\n",
      "epoch: 180 | train_loss: 0.7755 | val_loss:1.4435 | test_loss: 1.3813\n",
      "0.5369836695485111\n",
      "epoch: 181 | train_loss: 0.7723 | val_loss:1.4450 | test_loss: 1.3875\n",
      "0.5398655139289145\n",
      "epoch: 182 | train_loss: 0.7703 | val_loss:1.4462 | test_loss: 1.3892\n",
      "0.5350624399615754\n",
      "epoch: 183 | train_loss: 0.7690 | val_loss:1.4501 | test_loss: 1.3882\n",
      "0.5341018251681076\n",
      "epoch: 184 | train_loss: 0.7669 | val_loss:1.4509 | test_loss: 1.3955\n",
      "0.5379442843419788\n",
      "epoch: 185 | train_loss: 0.7643 | val_loss:1.4513 | test_loss: 1.3916\n",
      "0.5321805955811719\n",
      "epoch: 186 | train_loss: 0.7622 | val_loss:1.4568 | test_loss: 1.3973\n",
      "0.5408261287223823\n",
      "epoch: 187 | train_loss: 0.7608 | val_loss:1.4539 | test_loss: 1.3990\n",
      "0.5283381364073007\n",
      "epoch: 188 | train_loss: 0.7596 | val_loss:1.4654 | test_loss: 1.4028\n",
      "0.542747358309318\n",
      "epoch: 189 | train_loss: 0.7580 | val_loss:1.4562 | test_loss: 1.4025\n",
      "0.5302593659942363\n",
      "epoch: 190 | train_loss: 0.7568 | val_loss:1.4732 | test_loss: 1.4115\n",
      "0.5504322766570605\n",
      "epoch: 191 | train_loss: 0.7571 | val_loss:1.4600 | test_loss: 1.4043\n",
      "0.5302593659942363\n",
      "epoch: 192 | train_loss: 0.7606 | val_loss:1.4916 | test_loss: 1.4323\n",
      "0.5389048991354467\n",
      "epoch: 193 | train_loss: 0.7627 | val_loss:1.4698 | test_loss: 1.4117\n",
      "0.5302593659942363\n",
      "epoch: 194 | train_loss: 0.7599 | val_loss:1.5001 | test_loss: 1.4408\n",
      "0.5513928914505284\n",
      "epoch: 195 | train_loss: 0.7483 | val_loss:1.4671 | test_loss: 1.4115\n",
      "0.542747358309318\n",
      "epoch: 196 | train_loss: 0.7438 | val_loss:1.4750 | test_loss: 1.4140\n",
      "0.5369836695485111\n",
      "epoch: 197 | train_loss: 0.7466 | val_loss:1.4892 | test_loss: 1.4339\n",
      "0.5513928914505284\n",
      "epoch: 198 | train_loss: 0.7461 | val_loss:1.4746 | test_loss: 1.4169\n",
      "0.5331412103746398\n",
      "epoch: 199 | train_loss: 0.7412 | val_loss:1.4922 | test_loss: 1.4307\n",
      "0.542747358309318\n",
      "epoch: 200 | train_loss: 0.7386 | val_loss:1.4808 | test_loss: 1.4294\n",
      "0.5485110470701249\n",
      "epoch: 201 | train_loss: 0.7388 | val_loss:1.4862 | test_loss: 1.4236\n",
      "0.5360230547550432\n",
      "epoch: 202 | train_loss: 0.7356 | val_loss:1.4943 | test_loss: 1.4383\n",
      "0.5446685878962536\n",
      "epoch: 203 | train_loss: 0.7329 | val_loss:1.4827 | test_loss: 1.4298\n",
      "0.5369836695485111\n",
      "epoch: 204 | train_loss: 0.7334 | val_loss:1.4976 | test_loss: 1.4332\n",
      "0.5417867435158501\n",
      "epoch: 205 | train_loss: 0.7310 | val_loss:1.4938 | test_loss: 1.4432\n",
      "0.5446685878962536\n",
      "epoch: 206 | train_loss: 0.7267 | val_loss:1.4926 | test_loss: 1.4344\n",
      "0.542747358309318\n",
      "epoch: 207 | train_loss: 0.7252 | val_loss:1.4984 | test_loss: 1.4386\n",
      "0.5494716618635928\n",
      "epoch: 208 | train_loss: 0.7259 | val_loss:1.4962 | test_loss: 1.4462\n",
      "0.5437079731027857\n",
      "epoch: 209 | train_loss: 0.7244 | val_loss:1.5052 | test_loss: 1.4424\n",
      "0.5437079731027857\n",
      "epoch: 210 | train_loss: 0.7203 | val_loss:1.4986 | test_loss: 1.4454\n",
      "0.5456292026897214\n",
      "epoch: 211 | train_loss: 0.7187 | val_loss:1.5050 | test_loss: 1.4507\n",
      "0.547550432276657\n",
      "epoch: 212 | train_loss: 0.7186 | val_loss:1.5073 | test_loss: 1.4467\n",
      "0.5513928914505284\n",
      "epoch: 213 | train_loss: 0.7165 | val_loss:1.5057 | test_loss: 1.4544\n",
      "0.5417867435158501\n",
      "epoch: 214 | train_loss: 0.7141 | val_loss:1.5126 | test_loss: 1.4541\n",
      "0.5504322766570605\n",
      "epoch: 215 | train_loss: 0.7129 | val_loss:1.5076 | test_loss: 1.4519\n",
      "0.5417867435158501\n",
      "epoch: 216 | train_loss: 0.7120 | val_loss:1.5183 | test_loss: 1.4645\n",
      "0.5523535062439962\n",
      "epoch: 217 | train_loss: 0.7098 | val_loss:1.5139 | test_loss: 1.4561\n",
      "0.547550432276657\n",
      "epoch: 218 | train_loss: 0.7073 | val_loss:1.5162 | test_loss: 1.4618\n",
      "0.5437079731027857\n",
      "epoch: 219 | train_loss: 0.7059 | val_loss:1.5206 | test_loss: 1.4653\n",
      "0.553314121037464\n",
      "epoch: 220 | train_loss: 0.7053 | val_loss:1.5182 | test_loss: 1.4614\n",
      "0.5408261287223823\n",
      "epoch: 221 | train_loss: 0.7039 | val_loss:1.5281 | test_loss: 1.4737\n",
      "0.5513928914505284\n",
      "epoch: 222 | train_loss: 0.7017 | val_loss:1.5217 | test_loss: 1.4664\n",
      "0.5446685878962536\n",
      "epoch: 223 | train_loss: 0.6997 | val_loss:1.5295 | test_loss: 1.4727\n",
      "0.553314121037464\n",
      "epoch: 224 | train_loss: 0.6984 | val_loss:1.5273 | test_loss: 1.4749\n",
      "0.5494716618635928\n",
      "epoch: 225 | train_loss: 0.6973 | val_loss:1.5324 | test_loss: 1.4738\n",
      "0.553314121037464\n",
      "epoch: 226 | train_loss: 0.6957 | val_loss:1.5332 | test_loss: 1.4815\n",
      "0.5523535062439962\n",
      "epoch: 227 | train_loss: 0.6938 | val_loss:1.5354 | test_loss: 1.4782\n",
      "0.5523535062439962\n",
      "epoch: 228 | train_loss: 0.6918 | val_loss:1.5363 | test_loss: 1.4827\n",
      "0.5504322766570605\n",
      "epoch: 229 | train_loss: 0.6902 | val_loss:1.5395 | test_loss: 1.4852\n",
      "0.5523535062439962\n",
      "epoch: 230 | train_loss: 0.6890 | val_loss:1.5402 | test_loss: 1.4845\n",
      "0.5552353506243997\n",
      "epoch: 231 | train_loss: 0.6877 | val_loss:1.5445 | test_loss: 1.4922\n",
      "0.5542747358309318\n",
      "epoch: 232 | train_loss: 0.6864 | val_loss:1.5444 | test_loss: 1.4877\n",
      "0.5571565802113353\n",
      "epoch: 233 | train_loss: 0.6848 | val_loss:1.5484 | test_loss: 1.4966\n",
      "0.553314121037464\n",
      "epoch: 234 | train_loss: 0.6831 | val_loss:1.5488 | test_loss: 1.4925\n",
      "0.553314121037464\n",
      "epoch: 235 | train_loss: 0.6814 | val_loss:1.5517 | test_loss: 1.4995\n",
      "0.5571565802113353\n",
      "epoch: 236 | train_loss: 0.6797 | val_loss:1.5536 | test_loss: 1.4982\n",
      "0.553314121037464\n",
      "epoch: 237 | train_loss: 0.6781 | val_loss:1.5544 | test_loss: 1.5019\n",
      "0.5542747358309318\n",
      "epoch: 238 | train_loss: 0.6767 | val_loss:1.5594 | test_loss: 1.5044\n",
      "0.5552353506243997\n",
      "epoch: 239 | train_loss: 0.6754 | val_loss:1.5568 | test_loss: 1.5048\n",
      "0.5542747358309318\n",
      "epoch: 240 | train_loss: 0.6747 | val_loss:1.5687 | test_loss: 1.5128\n",
      "0.5600384245917387\n",
      "epoch: 241 | train_loss: 0.6752 | val_loss:1.5584 | test_loss: 1.5089\n",
      "0.547550432276657\n",
      "epoch: 242 | train_loss: 0.6794 | val_loss:1.5912 | test_loss: 1.5305\n",
      "0.5417867435158501\n",
      "epoch: 243 | train_loss: 0.6893 | val_loss:1.5680 | test_loss: 1.5249\n",
      "0.5408261287223823\n",
      "epoch: 244 | train_loss: 0.7084 | val_loss:1.6482 | test_loss: 1.5774\n",
      "0.5465898174831892\n",
      "epoch: 245 | train_loss: 0.6959 | val_loss:1.5813 | test_loss: 1.5418\n",
      "0.5523535062439962\n",
      "epoch: 246 | train_loss: 0.6699 | val_loss:1.5917 | test_loss: 1.5325\n",
      "0.5523535062439962\n",
      "epoch: 247 | train_loss: 0.6700 | val_loss:1.5942 | test_loss: 1.5363\n",
      "0.542747358309318\n",
      "epoch: 248 | train_loss: 0.6874 | val_loss:1.5755 | test_loss: 1.5327\n",
      "0.5485110470701249\n",
      "epoch: 249 | train_loss: 0.6807 | val_loss:1.6222 | test_loss: 1.5591\n",
      "0.5590778097982709\n",
      "epoch: 250 | train_loss: 0.6614 | val_loss:1.5850 | test_loss: 1.5322\n",
      "0.5542747358309318\n",
      "epoch: 251 | train_loss: 0.6751 | val_loss:1.5829 | test_loss: 1.5411\n",
      "0.5513928914505284\n",
      "epoch: 252 | train_loss: 0.6734 | val_loss:1.6206 | test_loss: 1.5580\n",
      "0.5581171950048031\n",
      "epoch: 253 | train_loss: 0.6571 | val_loss:1.5833 | test_loss: 1.5293\n",
      "0.5465898174831892\n",
      "epoch: 254 | train_loss: 0.6700 | val_loss:1.5820 | test_loss: 1.5407\n",
      "0.5571565802113353\n",
      "epoch: 255 | train_loss: 0.6682 | val_loss:1.6228 | test_loss: 1.5592\n",
      "0.5619596541786743\n",
      "epoch: 256 | train_loss: 0.6543 | val_loss:1.5975 | test_loss: 1.5466\n",
      "0.5465898174831892\n",
      "epoch: 257 | train_loss: 0.6660 | val_loss:1.5901 | test_loss: 1.5480\n",
      "0.5571565802113353\n",
      "epoch: 258 | train_loss: 0.6591 | val_loss:1.6167 | test_loss: 1.5583\n",
      "0.5629202689721422\n",
      "epoch: 259 | train_loss: 0.6513 | val_loss:1.6000 | test_loss: 1.5444\n",
      "0.5523535062439962\n",
      "epoch: 260 | train_loss: 0.6613 | val_loss:1.5920 | test_loss: 1.5512\n",
      "0.5600384245917387\n",
      "epoch: 261 | train_loss: 0.6522 | val_loss:1.6166 | test_loss: 1.5601\n",
      "0.5590778097982709\n",
      "epoch: 262 | train_loss: 0.6495 | val_loss:1.6143 | test_loss: 1.5590\n",
      "0.5609990393852066\n",
      "epoch: 263 | train_loss: 0.6549 | val_loss:1.5984 | test_loss: 1.5576\n",
      "0.5629202689721422\n",
      "epoch: 264 | train_loss: 0.6455 | val_loss:1.6086 | test_loss: 1.5535\n",
      "0.5590778097982709\n",
      "epoch: 265 | train_loss: 0.6472 | val_loss:1.6189 | test_loss: 1.5635\n",
      "0.5571565802113353\n",
      "epoch: 266 | train_loss: 0.6483 | val_loss:1.6006 | test_loss: 1.5560\n",
      "0.5600384245917387\n",
      "epoch: 267 | train_loss: 0.6413 | val_loss:1.6139 | test_loss: 1.5642\n",
      "0.5600384245917387\n",
      "epoch: 268 | train_loss: 0.6440 | val_loss:1.6266 | test_loss: 1.5690\n",
      "0.56388088376561\n",
      "epoch: 269 | train_loss: 0.6418 | val_loss:1.6086 | test_loss: 1.5647\n",
      "0.5648414985590778\n",
      "epoch: 270 | train_loss: 0.6382 | val_loss:1.6109 | test_loss: 1.5595\n",
      "0.5571565802113353\n",
      "epoch: 271 | train_loss: 0.6407 | val_loss:1.6312 | test_loss: 1.5771\n",
      "0.5600384245917387\n",
      "epoch: 272 | train_loss: 0.6374 | val_loss:1.6140 | test_loss: 1.5646\n",
      "0.5619596541786743\n",
      "epoch: 273 | train_loss: 0.6363 | val_loss:1.6219 | test_loss: 1.5773\n",
      "0.5590778097982709\n",
      "epoch: 274 | train_loss: 0.6381 | val_loss:1.6339 | test_loss: 1.5749\n",
      "0.5552353506243997\n",
      "epoch: 275 | train_loss: 0.6353 | val_loss:1.6270 | test_loss: 1.5827\n",
      "0.5609990393852066\n",
      "epoch: 276 | train_loss: 0.6363 | val_loss:1.6212 | test_loss: 1.5688\n",
      "0.5542747358309318\n",
      "epoch: 277 | train_loss: 0.6361 | val_loss:1.6443 | test_loss: 1.5952\n",
      "0.5619596541786743\n",
      "epoch: 278 | train_loss: 0.6322 | val_loss:1.6297 | test_loss: 1.5757\n",
      "0.5600384245917387\n",
      "epoch: 279 | train_loss: 0.6307 | val_loss:1.6317 | test_loss: 1.5885\n",
      "0.5629202689721422\n",
      "epoch: 280 | train_loss: 0.6283 | val_loss:1.6394 | test_loss: 1.5848\n",
      "0.5609990393852066\n",
      "epoch: 281 | train_loss: 0.6258 | val_loss:1.6354 | test_loss: 1.5848\n",
      "0.5619596541786743\n",
      "epoch: 282 | train_loss: 0.6265 | val_loss:1.6334 | test_loss: 1.5886\n",
      "0.5600384245917387\n",
      "epoch: 283 | train_loss: 0.6257 | val_loss:1.6432 | test_loss: 1.5880\n",
      "0.5609990393852066\n",
      "epoch: 284 | train_loss: 0.6247 | val_loss:1.6478 | test_loss: 1.6008\n",
      "0.5581171950048031\n",
      "epoch: 285 | train_loss: 0.6250 | val_loss:1.6387 | test_loss: 1.5887\n",
      "0.5600384245917387\n",
      "epoch: 286 | train_loss: 0.6232 | val_loss:1.6530 | test_loss: 1.6051\n",
      "0.5648414985590778\n",
      "epoch: 287 | train_loss: 0.6214 | val_loss:1.6473 | test_loss: 1.5932\n",
      "0.5696445725264169\n",
      "epoch: 288 | train_loss: 0.6202 | val_loss:1.6464 | test_loss: 1.6017\n",
      "0.5648414985590778\n",
      "epoch: 289 | train_loss: 0.6181 | val_loss:1.6528 | test_loss: 1.6010\n",
      "0.5648414985590778\n",
      "epoch: 290 | train_loss: 0.6168 | val_loss:1.6548 | test_loss: 1.6042\n",
      "0.5658021133525456\n",
      "epoch: 291 | train_loss: 0.6165 | val_loss:1.6519 | test_loss: 1.6061\n",
      "0.5658021133525456\n",
      "epoch: 292 | train_loss: 0.6155 | val_loss:1.6570 | test_loss: 1.6042\n",
      "0.5629202689721422\n",
      "epoch: 293 | train_loss: 0.6149 | val_loss:1.6627 | test_loss: 1.6147\n",
      "0.5619596541786743\n",
      "epoch: 294 | train_loss: 0.6147 | val_loss:1.6561 | test_loss: 1.6058\n",
      "0.5600384245917387\n",
      "epoch: 295 | train_loss: 0.6137 | val_loss:1.6689 | test_loss: 1.6219\n",
      "0.5648414985590778\n",
      "epoch: 296 | train_loss: 0.6129 | val_loss:1.6649 | test_loss: 1.6109\n",
      "0.5561959654178674\n",
      "epoch: 297 | train_loss: 0.6120 | val_loss:1.6678 | test_loss: 1.6237\n",
      "0.5658021133525456\n",
      "epoch: 298 | train_loss: 0.6108 | val_loss:1.6685 | test_loss: 1.6144\n",
      "0.5667627281460135\n",
      "epoch: 299 | train_loss: 0.6091 | val_loss:1.6744 | test_loss: 1.6272\n",
      "0.56388088376561\n",
      "epoch: 300 | train_loss: 0.6078 | val_loss:1.6684 | test_loss: 1.6187\n",
      "0.56388088376561\n",
      "epoch: 301 | train_loss: 0.6062 | val_loss:1.6773 | test_loss: 1.6284\n",
      "0.5667627281460135\n",
      "epoch: 302 | train_loss: 0.6049 | val_loss:1.6754 | test_loss: 1.6252\n",
      "0.56388088376561\n",
      "epoch: 303 | train_loss: 0.6040 | val_loss:1.6749 | test_loss: 1.6270\n",
      "0.5629202689721422\n",
      "epoch: 304 | train_loss: 0.6031 | val_loss:1.6817 | test_loss: 1.6319\n",
      "0.5677233429394812\n",
      "epoch: 305 | train_loss: 0.6021 | val_loss:1.6804 | test_loss: 1.6300\n",
      "0.5590778097982709\n",
      "epoch: 306 | train_loss: 0.6016 | val_loss:1.6838 | test_loss: 1.6377\n",
      "0.5677233429394812\n",
      "epoch: 307 | train_loss: 0.6012 | val_loss:1.6860 | test_loss: 1.6331\n",
      "0.5609990393852066\n",
      "epoch: 308 | train_loss: 0.6010 | val_loss:1.6912 | test_loss: 1.6460\n",
      "0.5648414985590778\n",
      "epoch: 309 | train_loss: 0.6016 | val_loss:1.6874 | test_loss: 1.6343\n",
      "0.5485110470701249\n",
      "epoch: 310 | train_loss: 0.6024 | val_loss:1.7030 | test_loss: 1.6586\n",
      "0.5658021133525456\n",
      "epoch: 311 | train_loss: 0.6035 | val_loss:1.6939 | test_loss: 1.6387\n",
      "0.5513928914505284\n",
      "epoch: 312 | train_loss: 0.6022 | val_loss:1.7071 | test_loss: 1.6649\n",
      "0.5658021133525456\n",
      "epoch: 313 | train_loss: 0.6000 | val_loss:1.6986 | test_loss: 1.6428\n",
      "0.5561959654178674\n",
      "epoch: 314 | train_loss: 0.5955 | val_loss:1.7027 | test_loss: 1.6578\n",
      "0.5658021133525456\n",
      "epoch: 315 | train_loss: 0.5926 | val_loss:1.6996 | test_loss: 1.6498\n",
      "0.5658021133525456\n",
      "epoch: 316 | train_loss: 0.5920 | val_loss:1.7029 | test_loss: 1.6519\n",
      "0.5561959654178674\n",
      "epoch: 317 | train_loss: 0.5927 | val_loss:1.7088 | test_loss: 1.6641\n",
      "0.56388088376561\n",
      "epoch: 318 | train_loss: 0.5937 | val_loss:1.7059 | test_loss: 1.6520\n",
      "0.5523535062439962\n",
      "epoch: 319 | train_loss: 0.5928 | val_loss:1.7175 | test_loss: 1.6729\n",
      "0.5658021133525456\n",
      "epoch: 320 | train_loss: 0.5909 | val_loss:1.7075 | test_loss: 1.6555\n",
      "0.5619596541786743\n",
      "epoch: 321 | train_loss: 0.5880 | val_loss:1.7166 | test_loss: 1.6697\n",
      "0.5658021133525456\n",
      "epoch: 322 | train_loss: 0.5861 | val_loss:1.7136 | test_loss: 1.6645\n",
      "0.5648414985590778\n",
      "epoch: 323 | train_loss: 0.5855 | val_loss:1.7139 | test_loss: 1.6647\n",
      "0.5609990393852066\n",
      "epoch: 324 | train_loss: 0.5855 | val_loss:1.7225 | test_loss: 1.6757\n",
      "0.5629202689721422\n",
      "epoch: 325 | train_loss: 0.5857 | val_loss:1.7175 | test_loss: 1.6657\n",
      "0.553314121037464\n",
      "epoch: 326 | train_loss: 0.5851 | val_loss:1.7276 | test_loss: 1.6830\n",
      "0.56388088376561\n",
      "epoch: 327 | train_loss: 0.5840 | val_loss:1.7232 | test_loss: 1.6705\n",
      "0.5581171950048031\n",
      "epoch: 328 | train_loss: 0.5821 | val_loss:1.7287 | test_loss: 1.6834\n",
      "0.5648414985590778\n",
      "epoch: 329 | train_loss: 0.5803 | val_loss:1.7263 | test_loss: 1.6760\n",
      "0.56388088376561\n",
      "epoch: 330 | train_loss: 0.5790 | val_loss:1.7303 | test_loss: 1.6819\n",
      "0.5619596541786743\n",
      "epoch: 331 | train_loss: 0.5782 | val_loss:1.7315 | test_loss: 1.6843\n",
      "0.5648414985590778\n",
      "epoch: 332 | train_loss: 0.5777 | val_loss:1.7331 | test_loss: 1.6825\n",
      "0.5590778097982709\n",
      "epoch: 333 | train_loss: 0.5774 | val_loss:1.7390 | test_loss: 1.6930\n",
      "0.5619596541786743\n",
      "epoch: 334 | train_loss: 0.5771 | val_loss:1.7354 | test_loss: 1.6842\n",
      "0.5571565802113353\n",
      "epoch: 335 | train_loss: 0.5764 | val_loss:1.7454 | test_loss: 1.6997\n",
      "0.5619596541786743\n",
      "epoch: 336 | train_loss: 0.5757 | val_loss:1.7394 | test_loss: 1.6881\n",
      "0.5571565802113353\n",
      "epoch: 337 | train_loss: 0.5745 | val_loss:1.7485 | test_loss: 1.7032\n",
      "0.5629202689721422\n",
      "epoch: 338 | train_loss: 0.5733 | val_loss:1.7442 | test_loss: 1.6929\n",
      "0.5590778097982709\n",
      "epoch: 339 | train_loss: 0.5720 | val_loss:1.7499 | test_loss: 1.7043\n",
      "0.5648414985590778\n",
      "epoch: 340 | train_loss: 0.5708 | val_loss:1.7487 | test_loss: 1.6984\n",
      "0.5600384245917387\n",
      "epoch: 341 | train_loss: 0.5696 | val_loss:1.7526 | test_loss: 1.7059\n",
      "0.5658021133525456\n",
      "epoch: 342 | train_loss: 0.5686 | val_loss:1.7530 | test_loss: 1.7044\n",
      "0.5667627281460135\n",
      "epoch: 343 | train_loss: 0.5676 | val_loss:1.7561 | test_loss: 1.7079\n",
      "0.5658021133525456\n",
      "epoch: 344 | train_loss: 0.5668 | val_loss:1.7573 | test_loss: 1.7100\n",
      "0.5658021133525456\n",
      "epoch: 345 | train_loss: 0.5660 | val_loss:1.7594 | test_loss: 1.7106\n",
      "0.5600384245917387\n",
      "epoch: 346 | train_loss: 0.5653 | val_loss:1.7632 | test_loss: 1.7166\n",
      "0.5658021133525456\n",
      "epoch: 347 | train_loss: 0.5647 | val_loss:1.7625 | test_loss: 1.7132\n",
      "0.5609990393852066\n",
      "epoch: 348 | train_loss: 0.5643 | val_loss:1.7697 | test_loss: 1.7241\n",
      "0.5619596541786743\n",
      "epoch: 349 | train_loss: 0.5645 | val_loss:1.7658 | test_loss: 1.7151\n",
      "0.5571565802113353\n",
      "epoch: 350 | train_loss: 0.5652 | val_loss:1.7786 | test_loss: 1.7352\n",
      "0.5725264169068204\n",
      "epoch: 351 | train_loss: 0.5675 | val_loss:1.7724 | test_loss: 1.7188\n",
      "0.5523535062439962\n",
      "epoch: 352 | train_loss: 0.5693 | val_loss:1.7913 | test_loss: 1.7511\n",
      "0.5706051873198847\n",
      "epoch: 353 | train_loss: 0.5719 | val_loss:1.7806 | test_loss: 1.7243\n",
      "0.5523535062439962\n",
      "epoch: 354 | train_loss: 0.5682 | val_loss:1.7960 | test_loss: 1.7558\n",
      "0.5696445725264169\n",
      "epoch: 355 | train_loss: 0.5632 | val_loss:1.7794 | test_loss: 1.7265\n",
      "0.5590778097982709\n",
      "epoch: 356 | train_loss: 0.5578 | val_loss:1.7862 | test_loss: 1.7406\n",
      "0.5609990393852066\n",
      "epoch: 357 | train_loss: 0.5563 | val_loss:1.7856 | test_loss: 1.7399\n",
      "0.5648414985590778\n",
      "epoch: 358 | train_loss: 0.5582 | val_loss:1.7858 | test_loss: 1.7344\n",
      "0.5542747358309318\n",
      "epoch: 359 | train_loss: 0.5601 | val_loss:1.8002 | test_loss: 1.7584\n",
      "0.5706051873198847\n",
      "epoch: 360 | train_loss: 0.5603 | val_loss:1.7897 | test_loss: 1.7366\n",
      "0.5571565802113353\n",
      "epoch: 361 | train_loss: 0.5564 | val_loss:1.8017 | test_loss: 1.7584\n",
      "0.5590778097982709\n",
      "epoch: 362 | train_loss: 0.5529 | val_loss:1.7920 | test_loss: 1.7441\n",
      "0.5648414985590778\n",
      "epoch: 363 | train_loss: 0.5514 | val_loss:1.7973 | test_loss: 1.7495\n",
      "0.5590778097982709\n",
      "epoch: 364 | train_loss: 0.5520 | val_loss:1.8022 | test_loss: 1.7588\n",
      "0.56388088376561\n",
      "epoch: 365 | train_loss: 0.5533 | val_loss:1.7985 | test_loss: 1.7475\n",
      "0.5561959654178674\n",
      "epoch: 366 | train_loss: 0.5529 | val_loss:1.8116 | test_loss: 1.7692\n",
      "0.5629202689721422\n",
      "epoch: 367 | train_loss: 0.5512 | val_loss:1.8015 | test_loss: 1.7518\n",
      "0.5581171950048031\n",
      "epoch: 368 | train_loss: 0.5485 | val_loss:1.8113 | test_loss: 1.7666\n",
      "0.5590778097982709\n",
      "epoch: 369 | train_loss: 0.5467 | val_loss:1.8067 | test_loss: 1.7605\n",
      "0.5600384245917387\n",
      "epoch: 370 | train_loss: 0.5462 | val_loss:1.8095 | test_loss: 1.7615\n",
      "0.5571565802113353\n",
      "epoch: 371 | train_loss: 0.5464 | val_loss:1.8165 | test_loss: 1.7726\n",
      "0.5619596541786743\n",
      "epoch: 372 | train_loss: 0.5466 | val_loss:1.8123 | test_loss: 1.7627\n",
      "0.5571565802113353\n",
      "epoch: 373 | train_loss: 0.5459 | val_loss:1.8238 | test_loss: 1.7804\n",
      "0.5629202689721422\n",
      "epoch: 374 | train_loss: 0.5446 | val_loss:1.8165 | test_loss: 1.7674\n",
      "0.5609990393852066\n",
      "epoch: 375 | train_loss: 0.5428 | val_loss:1.8245 | test_loss: 1.7797\n",
      "0.5561959654178674\n",
      "epoch: 376 | train_loss: 0.5414 | val_loss:1.8218 | test_loss: 1.7748\n",
      "0.5600384245917387\n",
      "epoch: 377 | train_loss: 0.5406 | val_loss:1.8247 | test_loss: 1.7778\n",
      "0.5600384245917387\n",
      "epoch: 378 | train_loss: 0.5402 | val_loss:1.8295 | test_loss: 1.7846\n",
      "0.5581171950048031\n",
      "epoch: 379 | train_loss: 0.5400 | val_loss:1.8273 | test_loss: 1.7790\n",
      "0.5581171950048031\n",
      "epoch: 380 | train_loss: 0.5397 | val_loss:1.8360 | test_loss: 1.7923\n",
      "0.5629202689721422\n",
      "epoch: 381 | train_loss: 0.5391 | val_loss:1.8312 | test_loss: 1.7823\n",
      "0.5590778097982709\n",
      "epoch: 382 | train_loss: 0.5382 | val_loss:1.8403 | test_loss: 1.7969\n",
      "0.5609990393852066\n",
      "epoch: 383 | train_loss: 0.5372 | val_loss:1.8359 | test_loss: 1.7872\n",
      "0.5600384245917387\n",
      "epoch: 384 | train_loss: 0.5361 | val_loss:1.8426 | test_loss: 1.7987\n",
      "0.5581171950048031\n",
      "epoch: 385 | train_loss: 0.5350 | val_loss:1.8408 | test_loss: 1.7927\n",
      "0.5581171950048031\n",
      "epoch: 386 | train_loss: 0.5340 | val_loss:1.8448 | test_loss: 1.8002\n",
      "0.5581171950048031\n",
      "epoch: 387 | train_loss: 0.5331 | val_loss:1.8466 | test_loss: 1.7993\n",
      "0.5609990393852066\n",
      "epoch: 388 | train_loss: 0.5323 | val_loss:1.8475 | test_loss: 1.8022\n",
      "0.5600384245917387\n",
      "epoch: 389 | train_loss: 0.5316 | val_loss:1.8521 | test_loss: 1.8053\n",
      "0.5600384245917387\n",
      "epoch: 390 | train_loss: 0.5309 | val_loss:1.8507 | test_loss: 1.8047\n",
      "0.5609990393852066\n",
      "epoch: 391 | train_loss: 0.5303 | val_loss:1.8581 | test_loss: 1.8121\n",
      "0.5561959654178674\n",
      "epoch: 392 | train_loss: 0.5298 | val_loss:1.8542 | test_loss: 1.8076\n",
      "0.5619596541786743\n",
      "epoch: 393 | train_loss: 0.5295 | val_loss:1.8647 | test_loss: 1.8195\n",
      "0.5590778097982709\n",
      "epoch: 394 | train_loss: 0.5295 | val_loss:1.8575 | test_loss: 1.8098\n",
      "0.5581171950048031\n",
      "epoch: 395 | train_loss: 0.5299 | val_loss:1.8732 | test_loss: 1.8294\n",
      "0.5658021133525456\n",
      "epoch: 396 | train_loss: 0.5311 | val_loss:1.8624 | test_loss: 1.8127\n",
      "0.5552353506243997\n",
      "epoch: 397 | train_loss: 0.5324 | val_loss:1.8841 | test_loss: 1.8426\n",
      "0.56388088376561\n",
      "epoch: 398 | train_loss: 0.5345 | val_loss:1.8695 | test_loss: 1.8171\n",
      "0.553314121037464\n",
      "epoch: 399 | train_loss: 0.5340 | val_loss:1.8917 | test_loss: 1.8522\n",
      "0.5648414985590778\n",
      "epoch: 400 | train_loss: 0.5327 | val_loss:1.8754 | test_loss: 1.8218\n",
      "0.5561959654178674\n",
      "epoch: 401 | train_loss: 0.5279 | val_loss:1.8856 | test_loss: 1.8456\n",
      "0.5677233429394812\n",
      "epoch: 402 | train_loss: 0.5242 | val_loss:1.8806 | test_loss: 1.8289\n",
      "0.5609990393852066\n",
      "epoch: 403 | train_loss: 0.5225 | val_loss:1.8767 | test_loss: 1.8347\n",
      "0.5629202689721422\n",
      "epoch: 404 | train_loss: 0.5234 | val_loss:1.8963 | test_loss: 1.8467\n",
      "0.5542747358309318\n",
      "epoch: 405 | train_loss: 0.5255 | val_loss:1.8764 | test_loss: 1.8336\n",
      "0.5590778097982709\n",
      "epoch: 406 | train_loss: 0.5283 | val_loss:1.9152 | test_loss: 1.8656\n",
      "0.5571565802113353\n",
      "epoch: 407 | train_loss: 0.5281 | val_loss:1.8803 | test_loss: 1.8392\n",
      "0.56388088376561\n",
      "epoch: 408 | train_loss: 0.5273 | val_loss:1.9209 | test_loss: 1.8694\n",
      "0.5571565802113353\n",
      "epoch: 409 | train_loss: 0.5231 | val_loss:1.8842 | test_loss: 1.8454\n",
      "0.5754082612872238\n",
      "epoch: 410 | train_loss: 0.5205 | val_loss:1.9092 | test_loss: 1.8556\n",
      "0.5609990393852066\n",
      "epoch: 411 | train_loss: 0.5195 | val_loss:1.8949 | test_loss: 1.8570\n",
      "0.5696445725264169\n",
      "epoch: 412 | train_loss: 0.5211 | val_loss:1.9040 | test_loss: 1.8492\n",
      "0.5590778097982709\n",
      "epoch: 413 | train_loss: 0.5217 | val_loss:1.9115 | test_loss: 1.8741\n",
      "0.5696445725264169\n",
      "epoch: 414 | train_loss: 0.5220 | val_loss:1.9053 | test_loss: 1.8514\n",
      "0.5590778097982709\n",
      "epoch: 415 | train_loss: 0.5181 | val_loss:1.9138 | test_loss: 1.8740\n",
      "0.5686839577329491\n",
      "epoch: 416 | train_loss: 0.5144 | val_loss:1.9061 | test_loss: 1.8564\n",
      "0.5619596541786743\n",
      "epoch: 417 | train_loss: 0.5121 | val_loss:1.9090 | test_loss: 1.8649\n",
      "0.5648414985590778\n",
      "epoch: 418 | train_loss: 0.5119 | val_loss:1.9157 | test_loss: 1.8713\n",
      "0.5658021133525456\n",
      "epoch: 419 | train_loss: 0.5131 | val_loss:1.9103 | test_loss: 1.8617\n",
      "0.5609990393852066\n",
      "epoch: 420 | train_loss: 0.5137 | val_loss:1.9243 | test_loss: 1.8830\n",
      "0.5754082612872238\n",
      "epoch: 421 | train_loss: 0.5132 | val_loss:1.9167 | test_loss: 1.8660\n",
      "0.5658021133525456\n",
      "epoch: 422 | train_loss: 0.5111 | val_loss:1.9241 | test_loss: 1.8838\n",
      "0.5725264169068204\n",
      "epoch: 423 | train_loss: 0.5093 | val_loss:1.9248 | test_loss: 1.8746\n",
      "0.5686839577329491\n",
      "epoch: 424 | train_loss: 0.5085 | val_loss:1.9191 | test_loss: 1.8779\n",
      "0.5696445725264169\n",
      "epoch: 425 | train_loss: 0.5087 | val_loss:1.9366 | test_loss: 1.8871\n",
      "0.5600384245917387\n",
      "epoch: 426 | train_loss: 0.5094 | val_loss:1.9197 | test_loss: 1.8785\n",
      "0.5696445725264169\n",
      "epoch: 427 | train_loss: 0.5103 | val_loss:1.9499 | test_loss: 1.9001\n",
      "0.56388088376561\n",
      "epoch: 428 | train_loss: 0.5098 | val_loss:1.9231 | test_loss: 1.8832\n",
      "0.5715658021133525\n",
      "epoch: 429 | train_loss: 0.5092 | val_loss:1.9539 | test_loss: 1.9024\n",
      "0.5677233429394812\n",
      "epoch: 430 | train_loss: 0.5075 | val_loss:1.9282 | test_loss: 1.8898\n",
      "0.574447646493756\n",
      "epoch: 431 | train_loss: 0.5067 | val_loss:1.9525 | test_loss: 1.8992\n",
      "0.5619596541786743\n",
      "epoch: 432 | train_loss: 0.5060 | val_loss:1.9373 | test_loss: 1.9003\n",
      "0.5725264169068204\n",
      "epoch: 433 | train_loss: 0.5065 | val_loss:1.9520 | test_loss: 1.8969\n",
      "0.5619596541786743\n",
      "epoch: 434 | train_loss: 0.5063 | val_loss:1.9479 | test_loss: 1.9116\n",
      "0.5686839577329491\n",
      "epoch: 435 | train_loss: 0.5065 | val_loss:1.9534 | test_loss: 1.8983\n",
      "0.5600384245917387\n",
      "epoch: 436 | train_loss: 0.5044 | val_loss:1.9540 | test_loss: 1.9163\n",
      "0.5725264169068204\n",
      "epoch: 437 | train_loss: 0.5025 | val_loss:1.9532 | test_loss: 1.9011\n",
      "0.5667627281460135\n",
      "epoch: 438 | train_loss: 0.4999 | val_loss:1.9546 | test_loss: 1.9132\n",
      "0.5706051873198847\n",
      "epoch: 439 | train_loss: 0.4982 | val_loss:1.9550 | test_loss: 1.9078\n",
      "0.5677233429394812\n",
      "epoch: 440 | train_loss: 0.4973 | val_loss:1.9561 | test_loss: 1.9105\n",
      "0.5677233429394812\n",
      "epoch: 441 | train_loss: 0.4973 | val_loss:1.9604 | test_loss: 1.9175\n",
      "0.5706051873198847\n",
      "epoch: 442 | train_loss: 0.4976 | val_loss:1.9609 | test_loss: 1.9116\n",
      "0.56388088376561\n",
      "epoch: 443 | train_loss: 0.4977 | val_loss:1.9662 | test_loss: 1.9263\n",
      "0.5715658021133525\n",
      "epoch: 444 | train_loss: 0.4979 | val_loss:1.9678 | test_loss: 1.9163\n",
      "0.5619596541786743\n",
      "epoch: 445 | train_loss: 0.4973 | val_loss:1.9697 | test_loss: 1.9315\n",
      "0.5696445725264169\n",
      "epoch: 446 | train_loss: 0.4969 | val_loss:1.9753 | test_loss: 1.9225\n",
      "0.56388088376561\n",
      "epoch: 447 | train_loss: 0.4960 | val_loss:1.9703 | test_loss: 1.9328\n",
      "0.5782901056676273\n",
      "epoch: 448 | train_loss: 0.4956 | val_loss:1.9841 | test_loss: 1.9309\n",
      "0.5686839577329491\n",
      "epoch: 449 | train_loss: 0.4950 | val_loss:1.9706 | test_loss: 1.9333\n",
      "0.5773294908741594\n",
      "epoch: 450 | train_loss: 0.4953 | val_loss:1.9946 | test_loss: 1.9413\n",
      "0.5696445725264169\n",
      "epoch: 451 | train_loss: 0.4953 | val_loss:1.9717 | test_loss: 1.9344\n",
      "0.5734870317002881\n",
      "epoch: 452 | train_loss: 0.4959 | val_loss:2.0046 | test_loss: 1.9516\n",
      "0.5677233429394812\n",
      "epoch: 453 | train_loss: 0.4952 | val_loss:1.9745 | test_loss: 1.9365\n",
      "0.5677233429394812\n",
      "epoch: 454 | train_loss: 0.4948 | val_loss:2.0090 | test_loss: 1.9576\n",
      "0.5658021133525456\n",
      "epoch: 455 | train_loss: 0.4925 | val_loss:1.9784 | test_loss: 1.9378\n",
      "0.5696445725264169\n",
      "epoch: 456 | train_loss: 0.4905 | val_loss:2.0035 | test_loss: 1.9554\n",
      "0.5667627281460135\n",
      "epoch: 457 | train_loss: 0.4886 | val_loss:1.9852 | test_loss: 1.9403\n",
      "0.5734870317002881\n",
      "epoch: 458 | train_loss: 0.4872 | val_loss:1.9969 | test_loss: 1.9531\n",
      "0.5686839577329491\n",
      "epoch: 459 | train_loss: 0.4868 | val_loss:1.9976 | test_loss: 1.9483\n",
      "0.5715658021133525\n",
      "epoch: 460 | train_loss: 0.4870 | val_loss:1.9948 | test_loss: 1.9553\n",
      "0.5773294908741594\n",
      "epoch: 461 | train_loss: 0.4877 | val_loss:2.0112 | test_loss: 1.9578\n",
      "0.5686839577329491\n",
      "epoch: 462 | train_loss: 0.4883 | val_loss:1.9973 | test_loss: 1.9610\n",
      "0.574447646493756\n",
      "epoch: 463 | train_loss: 0.4892 | val_loss:2.0221 | test_loss: 1.9659\n",
      "0.56388088376561\n",
      "epoch: 464 | train_loss: 0.4886 | val_loss:2.0030 | test_loss: 1.9684\n",
      "0.5754082612872238\n",
      "epoch: 465 | train_loss: 0.4883 | val_loss:2.0249 | test_loss: 1.9681\n",
      "0.5619596541786743\n",
      "epoch: 466 | train_loss: 0.4864 | val_loss:2.0098 | test_loss: 1.9738\n",
      "0.5734870317002881\n",
      "epoch: 467 | train_loss: 0.4852 | val_loss:2.0198 | test_loss: 1.9658\n",
      "0.5696445725264169\n",
      "epoch: 468 | train_loss: 0.4837 | val_loss:2.0198 | test_loss: 1.9801\n",
      "0.574447646493756\n",
      "epoch: 469 | train_loss: 0.4835 | val_loss:2.0136 | test_loss: 1.9646\n",
      "0.5715658021133525\n",
      "epoch: 470 | train_loss: 0.4839 | val_loss:2.0337 | test_loss: 1.9895\n",
      "0.5686839577329491\n",
      "epoch: 471 | train_loss: 0.4847 | val_loss:2.0122 | test_loss: 1.9679\n",
      "0.5658021133525456\n",
      "epoch: 472 | train_loss: 0.4853 | val_loss:2.0464 | test_loss: 1.9990\n",
      "0.56388088376561\n",
      "epoch: 473 | train_loss: 0.4844 | val_loss:2.0147 | test_loss: 1.9727\n",
      "0.5686839577329491\n",
      "epoch: 474 | train_loss: 0.4828 | val_loss:2.0475 | test_loss: 1.9991\n",
      "0.5667627281460135\n",
      "epoch: 475 | train_loss: 0.4798 | val_loss:2.0197 | test_loss: 1.9771\n",
      "0.5715658021133525\n",
      "epoch: 476 | train_loss: 0.4774 | val_loss:2.0390 | test_loss: 1.9916\n",
      "0.5734870317002881\n",
      "epoch: 477 | train_loss: 0.4757 | val_loss:2.0299 | test_loss: 1.9850\n",
      "0.5706051873198847\n",
      "epoch: 478 | train_loss: 0.4752 | val_loss:2.0326 | test_loss: 1.9870\n",
      "0.5706051873198847\n",
      "epoch: 479 | train_loss: 0.4754 | val_loss:2.0442 | test_loss: 1.9973\n",
      "0.5667627281460135\n",
      "epoch: 480 | train_loss: 0.4760 | val_loss:2.0323 | test_loss: 1.9881\n",
      "0.5696445725264169\n",
      "epoch: 481 | train_loss: 0.4769 | val_loss:2.0573 | test_loss: 2.0102\n",
      "0.5667627281460135\n",
      "epoch: 482 | train_loss: 0.4774 | val_loss:2.0356 | test_loss: 1.9907\n",
      "0.5686839577329491\n",
      "epoch: 483 | train_loss: 0.4778 | val_loss:2.0641 | test_loss: 2.0191\n",
      "0.574447646493756\n",
      "epoch: 484 | train_loss: 0.4778 | val_loss:2.0422 | test_loss: 1.9939\n",
      "0.5581171950048031\n",
      "epoch: 485 | train_loss: 0.4772 | val_loss:2.0642 | test_loss: 2.0232\n",
      "0.5725264169068204\n",
      "epoch: 486 | train_loss: 0.4771 | val_loss:2.0527 | test_loss: 1.9996\n",
      "0.5667627281460135\n",
      "epoch: 487 | train_loss: 0.4761 | val_loss:2.0583 | test_loss: 2.0218\n",
      "0.574447646493756\n",
      "epoch: 488 | train_loss: 0.4772 | val_loss:2.0698 | test_loss: 2.0115\n",
      "0.5648414985590778\n",
      "epoch: 489 | train_loss: 0.4779 | val_loss:2.0554 | test_loss: 2.0233\n",
      "0.5696445725264169\n",
      "epoch: 490 | train_loss: 0.4812 | val_loss:2.0933 | test_loss: 2.0313\n",
      "0.5619596541786743\n",
      "epoch: 491 | train_loss: 0.4797 | val_loss:2.0570 | test_loss: 2.0259\n",
      "0.5715658021133525\n",
      "epoch: 492 | train_loss: 0.4776 | val_loss:2.0958 | test_loss: 2.0365\n",
      "0.5696445725264169\n",
      "epoch: 493 | train_loss: 0.4711 | val_loss:2.0585 | test_loss: 2.0199\n",
      "0.5725264169068204\n",
      "epoch: 494 | train_loss: 0.4671 | val_loss:2.0736 | test_loss: 2.0261\n",
      "0.5811719500480308\n",
      "epoch: 495 | train_loss: 0.4673 | val_loss:2.0733 | test_loss: 2.0220\n",
      "0.5667627281460135\n",
      "epoch: 496 | train_loss: 0.4700 | val_loss:2.0671 | test_loss: 2.0315\n",
      "0.5677233429394812\n",
      "epoch: 497 | train_loss: 0.4730 | val_loss:2.0942 | test_loss: 2.0361\n",
      "0.5619596541786743\n",
      "epoch: 498 | train_loss: 0.4715 | val_loss:2.0713 | test_loss: 2.0387\n",
      "0.5773294908741594\n",
      "epoch: 499 | train_loss: 0.4691 | val_loss:2.0883 | test_loss: 2.0330\n",
      "0.5754082612872238\n",
      "epoch: 500 | train_loss: 0.4656 | val_loss:2.0785 | test_loss: 2.0397\n",
      "0.5734870317002881\n",
      "epoch: 501 | train_loss: 0.4642 | val_loss:2.0758 | test_loss: 2.0294\n",
      "0.5734870317002881\n",
      "epoch: 502 | train_loss: 0.4647 | val_loss:2.0956 | test_loss: 2.0482\n",
      "0.5725264169068204\n",
      "epoch: 503 | train_loss: 0.4653 | val_loss:2.0750 | test_loss: 2.0352\n",
      "0.5734870317002881\n",
      "epoch: 504 | train_loss: 0.4652 | val_loss:2.1049 | test_loss: 2.0539\n",
      "0.5734870317002881\n",
      "epoch: 505 | train_loss: 0.4632 | val_loss:2.0803 | test_loss: 2.0400\n",
      "0.5763688760806917\n",
      "epoch: 506 | train_loss: 0.4611 | val_loss:2.0984 | test_loss: 2.0502\n",
      "0.574447646493756\n",
      "epoch: 507 | train_loss: 0.4598 | val_loss:2.0909 | test_loss: 2.0452\n",
      "0.5830931796349664\n",
      "epoch: 508 | train_loss: 0.4595 | val_loss:2.0914 | test_loss: 2.0483\n",
      "0.5782901056676273\n",
      "epoch: 509 | train_loss: 0.4600 | val_loss:2.1051 | test_loss: 2.0546\n",
      "0.5763688760806917\n",
      "epoch: 510 | train_loss: 0.4601 | val_loss:2.0918 | test_loss: 2.0516\n",
      "0.5782901056676273\n",
      "epoch: 511 | train_loss: 0.4597 | val_loss:2.1120 | test_loss: 2.0605\n",
      "0.5821325648414986\n",
      "epoch: 512 | train_loss: 0.4584 | val_loss:2.0965 | test_loss: 2.0549\n",
      "0.5773294908741594\n",
      "epoch: 513 | train_loss: 0.4571 | val_loss:2.1102 | test_loss: 2.0611\n",
      "0.5773294908741594\n",
      "epoch: 514 | train_loss: 0.4562 | val_loss:2.1057 | test_loss: 2.0600\n",
      "0.579250720461095\n",
      "epoch: 515 | train_loss: 0.4557 | val_loss:2.1075 | test_loss: 2.0621\n",
      "0.5773294908741594\n",
      "epoch: 516 | train_loss: 0.4556 | val_loss:2.1162 | test_loss: 2.0667\n",
      "0.5811719500480308\n",
      "epoch: 517 | train_loss: 0.4555 | val_loss:2.1084 | test_loss: 2.0656\n",
      "0.5802113352545629\n",
      "epoch: 518 | train_loss: 0.4552 | val_loss:2.1232 | test_loss: 2.0721\n",
      "0.5802113352545629\n",
      "epoch: 519 | train_loss: 0.4545 | val_loss:2.1130 | test_loss: 2.0705\n",
      "0.5802113352545629\n",
      "epoch: 520 | train_loss: 0.4538 | val_loss:2.1243 | test_loss: 2.0736\n",
      "0.5802113352545629\n",
      "epoch: 521 | train_loss: 0.4531 | val_loss:2.1209 | test_loss: 2.0773\n",
      "0.579250720461095\n",
      "epoch: 522 | train_loss: 0.4529 | val_loss:2.1235 | test_loss: 2.0739\n",
      "0.5802113352545629\n",
      "epoch: 523 | train_loss: 0.4533 | val_loss:2.1320 | test_loss: 2.0878\n",
      "0.5821325648414986\n",
      "epoch: 524 | train_loss: 0.4551 | val_loss:2.1236 | test_loss: 2.0734\n",
      "0.5677233429394812\n",
      "epoch: 525 | train_loss: 0.4588 | val_loss:2.1508 | test_loss: 2.1083\n",
      "0.56388088376561\n",
      "epoch: 526 | train_loss: 0.4663 | val_loss:2.1335 | test_loss: 2.0799\n",
      "0.5552353506243997\n",
      "epoch: 527 | train_loss: 0.4724 | val_loss:2.1768 | test_loss: 2.1387\n",
      "0.5609990393852066\n",
      "epoch: 528 | train_loss: 0.4768 | val_loss:2.1456 | test_loss: 2.0881\n",
      "0.5590778097982709\n",
      "epoch: 529 | train_loss: 0.4633 | val_loss:2.1623 | test_loss: 2.1238\n",
      "0.5869356388088377\n",
      "epoch: 530 | train_loss: 0.4523 | val_loss:2.1414 | test_loss: 2.0873\n",
      "0.5754082612872238\n",
      "epoch: 531 | train_loss: 0.4498 | val_loss:2.1352 | test_loss: 2.0919\n",
      "0.574447646493756\n",
      "epoch: 532 | train_loss: 0.4560 | val_loss:2.1728 | test_loss: 2.1237\n",
      "0.5677233429394812\n",
      "epoch: 533 | train_loss: 0.4617 | val_loss:2.1397 | test_loss: 2.0913\n",
      "0.56388088376561\n",
      "epoch: 534 | train_loss: 0.4584 | val_loss:2.1764 | test_loss: 2.1305\n",
      "0.5821325648414986\n",
      "epoch: 535 | train_loss: 0.4499 | val_loss:2.1452 | test_loss: 2.0940\n",
      "0.5830931796349664\n",
      "epoch: 536 | train_loss: 0.4459 | val_loss:2.1519 | test_loss: 2.1062\n",
      "0.5802113352545629\n",
      "epoch: 537 | train_loss: 0.4482 | val_loss:2.1707 | test_loss: 2.1203\n",
      "0.5715658021133525\n",
      "epoch: 538 | train_loss: 0.4524 | val_loss:2.1448 | test_loss: 2.0982\n",
      "0.5734870317002881\n",
      "epoch: 539 | train_loss: 0.4523 | val_loss:2.1804 | test_loss: 2.1317\n",
      "0.5763688760806917\n",
      "epoch: 540 | train_loss: 0.4473 | val_loss:2.1510 | test_loss: 2.1033\n",
      "0.5840537944284342\n",
      "epoch: 541 | train_loss: 0.4437 | val_loss:2.1666 | test_loss: 2.1186\n",
      "0.5821325648414986\n",
      "epoch: 542 | train_loss: 0.4432 | val_loss:2.1673 | test_loss: 2.1191\n",
      "0.5773294908741594\n",
      "epoch: 543 | train_loss: 0.4455 | val_loss:2.1539 | test_loss: 2.1066\n",
      "0.5802113352545629\n",
      "epoch: 544 | train_loss: 0.4458 | val_loss:2.1801 | test_loss: 2.1314\n",
      "0.5773294908741594\n",
      "epoch: 545 | train_loss: 0.4437 | val_loss:2.1622 | test_loss: 2.1145\n",
      "0.5821325648414986\n",
      "epoch: 546 | train_loss: 0.4415 | val_loss:2.1781 | test_loss: 2.1290\n",
      "0.5821325648414986\n",
      "epoch: 547 | train_loss: 0.4403 | val_loss:2.1727 | test_loss: 2.1242\n",
      "0.5802113352545629\n",
      "epoch: 548 | train_loss: 0.4410 | val_loss:2.1677 | test_loss: 2.1188\n",
      "0.5802113352545629\n",
      "epoch: 549 | train_loss: 0.4412 | val_loss:2.1859 | test_loss: 2.1369\n",
      "0.5773294908741594\n",
      "epoch: 550 | train_loss: 0.4408 | val_loss:2.1752 | test_loss: 2.1258\n",
      "0.5811719500480308\n",
      "epoch: 551 | train_loss: 0.4395 | val_loss:2.1891 | test_loss: 2.1403\n",
      "0.5859750240153698\n",
      "epoch: 552 | train_loss: 0.4383 | val_loss:2.1799 | test_loss: 2.1296\n",
      "0.5840537944284342\n",
      "epoch: 553 | train_loss: 0.4376 | val_loss:2.1819 | test_loss: 2.1341\n",
      "0.5802113352545629\n",
      "epoch: 554 | train_loss: 0.4375 | val_loss:2.1934 | test_loss: 2.1418\n",
      "0.5763688760806917\n",
      "epoch: 555 | train_loss: 0.4376 | val_loss:2.1844 | test_loss: 2.1374\n",
      "0.5773294908741594\n",
      "epoch: 556 | train_loss: 0.4374 | val_loss:2.2009 | test_loss: 2.1481\n",
      "0.5811719500480308\n",
      "epoch: 557 | train_loss: 0.4369 | val_loss:2.1852 | test_loss: 2.1392\n",
      "0.5821325648414986\n",
      "epoch: 558 | train_loss: 0.4363 | val_loss:2.2040 | test_loss: 2.1499\n",
      "0.5802113352545629\n",
      "epoch: 559 | train_loss: 0.4360 | val_loss:2.1945 | test_loss: 2.1499\n",
      "0.5811719500480308\n",
      "epoch: 560 | train_loss: 0.4364 | val_loss:2.2078 | test_loss: 2.1512\n",
      "0.5773294908741594\n",
      "epoch: 561 | train_loss: 0.4374 | val_loss:2.2005 | test_loss: 2.1590\n",
      "0.5859750240153698\n",
      "epoch: 562 | train_loss: 0.4401 | val_loss:2.2167 | test_loss: 2.1553\n",
      "0.5715658021133525\n",
      "epoch: 563 | train_loss: 0.4427 | val_loss:2.2102 | test_loss: 2.1741\n",
      "0.5725264169068204\n",
      "epoch: 564 | train_loss: 0.4490 | val_loss:2.2398 | test_loss: 2.1714\n",
      "0.5658021133525456\n",
      "epoch: 565 | train_loss: 0.4499 | val_loss:2.2158 | test_loss: 2.1852\n",
      "0.5706051873198847\n",
      "epoch: 566 | train_loss: 0.4546 | val_loss:2.2581 | test_loss: 2.1857\n",
      "0.5677233429394812\n",
      "epoch: 567 | train_loss: 0.4473 | val_loss:2.2152 | test_loss: 2.1833\n",
      "0.574447646493756\n",
      "epoch: 568 | train_loss: 0.4408 | val_loss:2.2483 | test_loss: 2.1840\n",
      "0.5821325648414986\n",
      "epoch: 569 | train_loss: 0.4322 | val_loss:2.2115 | test_loss: 2.1656\n",
      "0.5821325648414986\n",
      "epoch: 570 | train_loss: 0.4313 | val_loss:2.2195 | test_loss: 2.1727\n",
      "0.5811719500480308\n",
      "epoch: 571 | train_loss: 0.4363 | val_loss:2.2379 | test_loss: 2.1753\n",
      "0.5686839577329491\n",
      "epoch: 572 | train_loss: 0.4396 | val_loss:2.2225 | test_loss: 2.1871\n",
      "0.5802113352545629\n",
      "epoch: 573 | train_loss: 0.4398 | val_loss:2.2535 | test_loss: 2.1883\n",
      "0.5782901056676273\n",
      "epoch: 574 | train_loss: 0.4326 | val_loss:2.2212 | test_loss: 2.1809\n",
      "0.5811719500480308\n",
      "epoch: 575 | train_loss: 0.4282 | val_loss:2.2284 | test_loss: 2.1755\n",
      "0.5850144092219021\n",
      "epoch: 576 | train_loss: 0.4282 | val_loss:2.2360 | test_loss: 2.1815\n",
      "0.5763688760806917\n",
      "epoch: 577 | train_loss: 0.4310 | val_loss:2.2280 | test_loss: 2.1872\n",
      "0.5802113352545629\n",
      "epoch: 578 | train_loss: 0.4332 | val_loss:2.2537 | test_loss: 2.1915\n",
      "0.5782901056676273\n",
      "epoch: 579 | train_loss: 0.4310 | val_loss:2.2330 | test_loss: 2.1927\n",
      "0.5859750240153698\n",
      "epoch: 580 | train_loss: 0.4281 | val_loss:2.2438 | test_loss: 2.1860\n",
      "0.5830931796349664\n",
      "epoch: 581 | train_loss: 0.4259 | val_loss:2.2447 | test_loss: 2.1952\n",
      "0.5811719500480308\n",
      "epoch: 582 | train_loss: 0.4258 | val_loss:2.2367 | test_loss: 2.1885\n",
      "0.5830931796349664\n",
      "epoch: 583 | train_loss: 0.4268 | val_loss:2.2582 | test_loss: 2.1999\n",
      "0.5821325648414986\n",
      "epoch: 584 | train_loss: 0.4269 | val_loss:2.2385 | test_loss: 2.1946\n",
      "0.5773294908741594\n",
      "epoch: 585 | train_loss: 0.4260 | val_loss:2.2602 | test_loss: 2.2005\n",
      "0.5821325648414986\n",
      "epoch: 586 | train_loss: 0.4242 | val_loss:2.2475 | test_loss: 2.2001\n",
      "0.5869356388088377\n",
      "epoch: 587 | train_loss: 0.4230 | val_loss:2.2515 | test_loss: 2.1967\n",
      "0.5830931796349664\n",
      "epoch: 588 | train_loss: 0.4227 | val_loss:2.2597 | test_loss: 2.2055\n",
      "0.5773294908741594\n",
      "epoch: 589 | train_loss: 0.4230 | val_loss:2.2492 | test_loss: 2.1997\n",
      "0.5830931796349664\n",
      "epoch: 590 | train_loss: 0.4232 | val_loss:2.2719 | test_loss: 2.2135\n",
      "0.5821325648414986\n",
      "epoch: 591 | train_loss: 0.4225 | val_loss:2.2533 | test_loss: 2.2048\n",
      "0.5830931796349664\n",
      "epoch: 592 | train_loss: 0.4214 | val_loss:2.2707 | test_loss: 2.2131\n",
      "0.5830931796349664\n",
      "epoch: 593 | train_loss: 0.4203 | val_loss:2.2617 | test_loss: 2.2099\n",
      "0.5859750240153698\n",
      "epoch: 594 | train_loss: 0.4196 | val_loss:2.2674 | test_loss: 2.2136\n",
      "0.5859750240153698\n",
      "epoch: 595 | train_loss: 0.4194 | val_loss:2.2729 | test_loss: 2.2171\n",
      "0.5821325648414986\n",
      "epoch: 596 | train_loss: 0.4194 | val_loss:2.2653 | test_loss: 2.2147\n",
      "0.5821325648414986\n",
      "epoch: 597 | train_loss: 0.4193 | val_loss:2.2807 | test_loss: 2.2229\n",
      "0.5811719500480308\n",
      "epoch: 598 | train_loss: 0.4188 | val_loss:2.2693 | test_loss: 2.2189\n",
      "0.5840537944284342\n",
      "epoch: 599 | train_loss: 0.4182 | val_loss:2.2844 | test_loss: 2.2273\n",
      "0.5878962536023055\n",
      "epoch: 600 | train_loss: 0.4174 | val_loss:2.2747 | test_loss: 2.2222\n",
      "0.5869356388088377\n",
      "epoch: 601 | train_loss: 0.4168 | val_loss:2.2824 | test_loss: 2.2279\n",
      "0.5850144092219021\n",
      "epoch: 602 | train_loss: 0.4163 | val_loss:2.2849 | test_loss: 2.2287\n",
      "0.5840537944284342\n",
      "epoch: 603 | train_loss: 0.4162 | val_loss:2.2839 | test_loss: 2.2319\n",
      "0.5821325648414986\n",
      "epoch: 604 | train_loss: 0.4160 | val_loss:2.2930 | test_loss: 2.2340\n",
      "0.5802113352545629\n",
      "epoch: 605 | train_loss: 0.4159 | val_loss:2.2844 | test_loss: 2.2336\n",
      "0.5821325648414986\n",
      "epoch: 606 | train_loss: 0.4156 | val_loss:2.3001 | test_loss: 2.2397\n",
      "0.5840537944284342\n",
      "epoch: 607 | train_loss: 0.4151 | val_loss:2.2906 | test_loss: 2.2395\n",
      "0.5821325648414986\n",
      "epoch: 608 | train_loss: 0.4145 | val_loss:2.3030 | test_loss: 2.2429\n",
      "0.5802113352545629\n",
      "epoch: 609 | train_loss: 0.4138 | val_loss:2.2943 | test_loss: 2.2418\n",
      "0.5830931796349664\n",
      "epoch: 610 | train_loss: 0.4131 | val_loss:2.3033 | test_loss: 2.2444\n",
      "0.5850144092219021\n",
      "epoch: 611 | train_loss: 0.4126 | val_loss:2.3037 | test_loss: 2.2492\n",
      "0.5840537944284342\n",
      "epoch: 612 | train_loss: 0.4123 | val_loss:2.3037 | test_loss: 2.2462\n",
      "0.5821325648414986\n",
      "epoch: 613 | train_loss: 0.4124 | val_loss:2.3126 | test_loss: 2.2566\n",
      "0.5830931796349664\n",
      "epoch: 614 | train_loss: 0.4132 | val_loss:2.3029 | test_loss: 2.2460\n",
      "0.5782901056676273\n",
      "epoch: 615 | train_loss: 0.4144 | val_loss:2.3256 | test_loss: 2.2698\n",
      "0.579250720461095\n",
      "epoch: 616 | train_loss: 0.4172 | val_loss:2.3077 | test_loss: 2.2490\n",
      "0.5706051873198847\n",
      "epoch: 617 | train_loss: 0.4204 | val_loss:2.3406 | test_loss: 2.2880\n",
      "0.5763688760806917\n",
      "epoch: 618 | train_loss: 0.4260 | val_loss:2.3186 | test_loss: 2.2554\n",
      "0.5706051873198847\n",
      "epoch: 619 | train_loss: 0.4265 | val_loss:2.3501 | test_loss: 2.3023\n",
      "0.5734870317002881\n",
      "epoch: 620 | train_loss: 0.4273 | val_loss:2.3314 | test_loss: 2.2632\n",
      "0.5734870317002881\n",
      "epoch: 621 | train_loss: 0.4193 | val_loss:2.3350 | test_loss: 2.2899\n",
      "0.5802113352545629\n",
      "epoch: 622 | train_loss: 0.4157 | val_loss:2.3440 | test_loss: 2.2737\n",
      "0.579250720461095\n",
      "epoch: 623 | train_loss: 0.4145 | val_loss:2.3188 | test_loss: 2.2736\n",
      "0.5811719500480308\n",
      "epoch: 624 | train_loss: 0.4168 | val_loss:2.3648 | test_loss: 2.2961\n",
      "0.5763688760806917\n",
      "epoch: 625 | train_loss: 0.4171 | val_loss:2.3190 | test_loss: 2.2700\n",
      "0.5773294908741594\n",
      "epoch: 626 | train_loss: 0.4154 | val_loss:2.3662 | test_loss: 2.3033\n",
      "0.5811719500480308\n",
      "epoch: 627 | train_loss: 0.4118 | val_loss:2.3281 | test_loss: 2.2694\n",
      "0.579250720461095\n",
      "epoch: 628 | train_loss: 0.4093 | val_loss:2.3438 | test_loss: 2.2907\n",
      "0.5821325648414986\n",
      "epoch: 629 | train_loss: 0.4099 | val_loss:2.3511 | test_loss: 2.2827\n",
      "0.5811719500480308\n",
      "epoch: 630 | train_loss: 0.4117 | val_loss:2.3315 | test_loss: 2.2862\n",
      "0.5782901056676273\n",
      "epoch: 631 | train_loss: 0.4136 | val_loss:2.3717 | test_loss: 2.3003\n",
      "0.5773294908741594\n",
      "epoch: 632 | train_loss: 0.4111 | val_loss:2.3333 | test_loss: 2.2875\n",
      "0.5782901056676273\n",
      "epoch: 633 | train_loss: 0.4072 | val_loss:2.3627 | test_loss: 2.2959\n",
      "0.5840537944284342\n",
      "epoch: 634 | train_loss: 0.4038 | val_loss:2.3450 | test_loss: 2.2902\n",
      "0.5821325648414986\n",
      "epoch: 635 | train_loss: 0.4039 | val_loss:2.3415 | test_loss: 2.2848\n",
      "0.5878962536023055\n",
      "epoch: 636 | train_loss: 0.4061 | val_loss:2.3705 | test_loss: 2.3064\n",
      "0.5763688760806917\n",
      "epoch: 637 | train_loss: 0.4073 | val_loss:2.3424 | test_loss: 2.2915\n",
      "0.5850144092219021\n",
      "epoch: 638 | train_loss: 0.4062 | val_loss:2.3779 | test_loss: 2.3123\n",
      "0.5802113352545629\n",
      "epoch: 639 | train_loss: 0.4032 | val_loss:2.3473 | test_loss: 2.2934\n",
      "0.5878962536023055\n",
      "epoch: 640 | train_loss: 0.4011 | val_loss:2.3615 | test_loss: 2.3018\n",
      "0.5869356388088377\n",
      "epoch: 641 | train_loss: 0.4009 | val_loss:2.3666 | test_loss: 2.3051\n",
      "0.5821325648414986\n",
      "epoch: 642 | train_loss: 0.4018 | val_loss:2.3557 | test_loss: 2.3017\n",
      "0.5869356388088377\n",
      "epoch: 643 | train_loss: 0.4024 | val_loss:2.3814 | test_loss: 2.3158\n",
      "0.5811719500480308\n",
      "epoch: 644 | train_loss: 0.4013 | val_loss:2.3601 | test_loss: 2.3063\n",
      "0.5859750240153698\n",
      "epoch: 645 | train_loss: 0.3998 | val_loss:2.3784 | test_loss: 2.3141\n",
      "0.5821325648414986\n",
      "epoch: 646 | train_loss: 0.3988 | val_loss:2.3727 | test_loss: 2.3142\n",
      "0.5869356388088377\n",
      "epoch: 647 | train_loss: 0.3989 | val_loss:2.3702 | test_loss: 2.3098\n",
      "0.5878962536023055\n",
      "epoch: 648 | train_loss: 0.3995 | val_loss:2.3882 | test_loss: 2.3259\n",
      "0.5811719500480308\n",
      "epoch: 649 | train_loss: 0.3997 | val_loss:2.3721 | test_loss: 2.3129\n",
      "0.5850144092219021\n",
      "epoch: 650 | train_loss: 0.3993 | val_loss:2.3944 | test_loss: 2.3327\n",
      "0.5811719500480308\n",
      "epoch: 651 | train_loss: 0.3987 | val_loss:2.3782 | test_loss: 2.3159\n",
      "0.5811719500480308\n",
      "epoch: 652 | train_loss: 0.3984 | val_loss:2.3903 | test_loss: 2.3332\n",
      "0.5821325648414986\n",
      "epoch: 653 | train_loss: 0.3990 | val_loss:2.3933 | test_loss: 2.3251\n",
      "0.5782901056676273\n",
      "epoch: 654 | train_loss: 0.4000 | val_loss:2.3890 | test_loss: 2.3374\n",
      "0.5840537944284342\n",
      "epoch: 655 | train_loss: 0.4025 | val_loss:2.4082 | test_loss: 2.3342\n",
      "0.5773294908741594\n",
      "epoch: 656 | train_loss: 0.4034 | val_loss:2.3932 | test_loss: 2.3456\n",
      "0.5840537944284342\n",
      "epoch: 657 | train_loss: 0.4059 | val_loss:2.4194 | test_loss: 2.3421\n",
      "0.5706051873198847\n",
      "epoch: 658 | train_loss: 0.4040 | val_loss:2.4054 | test_loss: 2.3566\n",
      "0.5840537944284342\n",
      "epoch: 659 | train_loss: 0.4043 | val_loss:2.4115 | test_loss: 2.3373\n",
      "0.5773294908741594\n",
      "epoch: 660 | train_loss: 0.4014 | val_loss:2.4160 | test_loss: 2.3611\n",
      "0.5773294908741594\n",
      "epoch: 661 | train_loss: 0.4005 | val_loss:2.3983 | test_loss: 2.3332\n",
      "0.5821325648414986\n",
      "epoch: 662 | train_loss: 0.3986 | val_loss:2.4286 | test_loss: 2.3658\n",
      "0.5850144092219021\n",
      "epoch: 663 | train_loss: 0.3966 | val_loss:2.3965 | test_loss: 2.3386\n",
      "0.5888568683957733\n",
      "epoch: 664 | train_loss: 0.3943 | val_loss:2.4228 | test_loss: 2.3570\n",
      "0.5773294908741594\n",
      "epoch: 665 | train_loss: 0.3923 | val_loss:2.4055 | test_loss: 2.3485\n",
      "0.5869356388088377\n",
      "epoch: 666 | train_loss: 0.3925 | val_loss:2.4124 | test_loss: 2.3472\n",
      "0.5830931796349664\n",
      "epoch: 667 | train_loss: 0.3942 | val_loss:2.4253 | test_loss: 2.3664\n",
      "0.5782901056676273\n",
      "epoch: 668 | train_loss: 0.3960 | val_loss:2.4127 | test_loss: 2.3477\n",
      "0.5773294908741594\n",
      "epoch: 669 | train_loss: 0.3962 | val_loss:2.4382 | test_loss: 2.3788\n",
      "0.579250720461095\n",
      "epoch: 670 | train_loss: 0.3965 | val_loss:2.4170 | test_loss: 2.3507\n",
      "0.5821325648414986\n",
      "epoch: 671 | train_loss: 0.3944 | val_loss:2.4355 | test_loss: 2.3772\n",
      "0.5859750240153698\n",
      "epoch: 672 | train_loss: 0.3923 | val_loss:2.4280 | test_loss: 2.3591\n",
      "0.5821325648414986\n",
      "epoch: 673 | train_loss: 0.3905 | val_loss:2.4267 | test_loss: 2.3699\n",
      "0.5850144092219021\n",
      "epoch: 674 | train_loss: 0.3901 | val_loss:2.4361 | test_loss: 2.3661\n",
      "0.5830931796349664\n",
      "epoch: 675 | train_loss: 0.3897 | val_loss:2.4220 | test_loss: 2.3654\n",
      "0.5840537944284342\n",
      "epoch: 676 | train_loss: 0.3895 | val_loss:2.4486 | test_loss: 2.3791\n",
      "0.5869356388088377\n",
      "epoch: 677 | train_loss: 0.3884 | val_loss:2.4282 | test_loss: 2.3687\n",
      "0.5898174831892411\n",
      "epoch: 678 | train_loss: 0.3874 | val_loss:2.4429 | test_loss: 2.3767\n",
      "0.5859750240153698\n",
      "epoch: 679 | train_loss: 0.3865 | val_loss:2.4361 | test_loss: 2.3718\n",
      "0.5859750240153698\n",
      "epoch: 680 | train_loss: 0.3862 | val_loss:2.4416 | test_loss: 2.3795\n",
      "0.5850144092219021\n",
      "epoch: 681 | train_loss: 0.3863 | val_loss:2.4499 | test_loss: 2.3812\n",
      "0.5840537944284342\n",
      "epoch: 682 | train_loss: 0.3865 | val_loss:2.4386 | test_loss: 2.3801\n",
      "0.5850144092219021\n",
      "epoch: 683 | train_loss: 0.3867 | val_loss:2.4582 | test_loss: 2.3871\n",
      "0.5830931796349664\n",
      "epoch: 684 | train_loss: 0.3866 | val_loss:2.4447 | test_loss: 2.3875\n",
      "0.5869356388088377\n",
      "epoch: 685 | train_loss: 0.3863 | val_loss:2.4617 | test_loss: 2.3900\n",
      "0.5811719500480308\n",
      "epoch: 686 | train_loss: 0.3860 | val_loss:2.4487 | test_loss: 2.3917\n",
      "0.5859750240153698\n",
      "epoch: 687 | train_loss: 0.3860 | val_loss:2.4628 | test_loss: 2.3910\n",
      "0.5850144092219021\n",
      "epoch: 688 | train_loss: 0.3860 | val_loss:2.4609 | test_loss: 2.4031\n",
      "0.5830931796349664\n",
      "epoch: 689 | train_loss: 0.3869 | val_loss:2.4636 | test_loss: 2.3916\n",
      "0.579250720461095\n",
      "epoch: 690 | train_loss: 0.3877 | val_loss:2.4704 | test_loss: 2.4125\n",
      "0.5773294908741594\n",
      "epoch: 691 | train_loss: 0.3898 | val_loss:2.4669 | test_loss: 2.3942\n",
      "0.5763688760806917\n",
      "epoch: 692 | train_loss: 0.3901 | val_loss:2.4830 | test_loss: 2.4253\n",
      "0.5754082612872238\n",
      "epoch: 693 | train_loss: 0.3912 | val_loss:2.4732 | test_loss: 2.3993\n",
      "0.5811719500480308\n",
      "epoch: 694 | train_loss: 0.3889 | val_loss:2.4830 | test_loss: 2.4257\n",
      "0.579250720461095\n",
      "epoch: 695 | train_loss: 0.3872 | val_loss:2.4777 | test_loss: 2.4033\n",
      "0.5821325648414986\n",
      "epoch: 696 | train_loss: 0.3842 | val_loss:2.4779 | test_loss: 2.4200\n",
      "0.5859750240153698\n",
      "epoch: 697 | train_loss: 0.3825 | val_loss:2.4858 | test_loss: 2.4121\n",
      "0.5821325648414986\n",
      "epoch: 698 | train_loss: 0.3813 | val_loss:2.4713 | test_loss: 2.4121\n",
      "0.5859750240153698\n",
      "epoch: 699 | train_loss: 0.3810 | val_loss:2.4934 | test_loss: 2.4215\n",
      "0.5888568683957733\n",
      "epoch: 700 | train_loss: 0.3806 | val_loss:2.4760 | test_loss: 2.4138\n",
      "0.590778097982709\n",
      "epoch: 701 | train_loss: 0.3800 | val_loss:2.4971 | test_loss: 2.4283\n",
      "0.5811719500480308\n",
      "epoch: 702 | train_loss: 0.3796 | val_loss:2.4794 | test_loss: 2.4135\n",
      "0.590778097982709\n",
      "epoch: 703 | train_loss: 0.3792 | val_loss:2.4948 | test_loss: 2.4304\n",
      "0.5869356388088377\n",
      "epoch: 704 | train_loss: 0.3793 | val_loss:2.4907 | test_loss: 2.4207\n",
      "0.5869356388088377\n",
      "epoch: 705 | train_loss: 0.3794 | val_loss:2.4937 | test_loss: 2.4334\n",
      "0.5830931796349664\n",
      "epoch: 706 | train_loss: 0.3801 | val_loss:2.5009 | test_loss: 2.4268\n",
      "0.5830931796349664\n",
      "epoch: 707 | train_loss: 0.3807 | val_loss:2.4932 | test_loss: 2.4371\n",
      "0.5840537944284342\n",
      "epoch: 708 | train_loss: 0.3824 | val_loss:2.5161 | test_loss: 2.4376\n",
      "0.5811719500480308\n",
      "epoch: 709 | train_loss: 0.3830 | val_loss:2.4964 | test_loss: 2.4434\n",
      "0.5821325648414986\n",
      "epoch: 710 | train_loss: 0.3850 | val_loss:2.5275 | test_loss: 2.4456\n",
      "0.5763688760806917\n",
      "epoch: 711 | train_loss: 0.3840 | val_loss:2.5012 | test_loss: 2.4493\n",
      "0.5830931796349664\n",
      "epoch: 712 | train_loss: 0.3842 | val_loss:2.5331 | test_loss: 2.4507\n",
      "0.5811719500480308\n",
      "epoch: 713 | train_loss: 0.3807 | val_loss:2.5061 | test_loss: 2.4511\n",
      "0.5888568683957733\n",
      "epoch: 714 | train_loss: 0.3781 | val_loss:2.5233 | test_loss: 2.4459\n",
      "0.590778097982709\n",
      "epoch: 715 | train_loss: 0.3753 | val_loss:2.5130 | test_loss: 2.4506\n",
      "0.5850144092219021\n",
      "epoch: 716 | train_loss: 0.3741 | val_loss:2.5114 | test_loss: 2.4429\n",
      "0.5898174831892411\n",
      "epoch: 717 | train_loss: 0.3743 | val_loss:2.5275 | test_loss: 2.4573\n",
      "0.5869356388088377\n",
      "epoch: 718 | train_loss: 0.3752 | val_loss:2.5068 | test_loss: 2.4460\n",
      "0.5869356388088377\n",
      "epoch: 719 | train_loss: 0.3762 | val_loss:2.5413 | test_loss: 2.4662\n",
      "0.5898174831892411\n",
      "epoch: 720 | train_loss: 0.3758 | val_loss:2.5107 | test_loss: 2.4524\n",
      "0.5859750240153698\n",
      "epoch: 721 | train_loss: 0.3749 | val_loss:2.5428 | test_loss: 2.4672\n",
      "0.5878962536023055\n",
      "epoch: 722 | train_loss: 0.3731 | val_loss:2.5174 | test_loss: 2.4565\n",
      "0.5859750240153698\n",
      "epoch: 723 | train_loss: 0.3718 | val_loss:2.5357 | test_loss: 2.4629\n",
      "0.5898174831892411\n",
      "epoch: 724 | train_loss: 0.3709 | val_loss:2.5297 | test_loss: 2.4638\n",
      "0.5898174831892411\n",
      "epoch: 725 | train_loss: 0.3707 | val_loss:2.5290 | test_loss: 2.4601\n",
      "0.5917387127761767\n",
      "epoch: 726 | train_loss: 0.3712 | val_loss:2.5451 | test_loss: 2.4751\n",
      "0.5830931796349664\n",
      "epoch: 727 | train_loss: 0.3721 | val_loss:2.5278 | test_loss: 2.4616\n",
      "0.5898174831892411\n",
      "epoch: 728 | train_loss: 0.3733 | val_loss:2.5589 | test_loss: 2.4875\n",
      "0.5830931796349664\n",
      "epoch: 729 | train_loss: 0.3743 | val_loss:2.5306 | test_loss: 2.4638\n",
      "0.5830931796349664\n",
      "epoch: 730 | train_loss: 0.3754 | val_loss:2.5676 | test_loss: 2.4981\n",
      "0.5763688760806917\n",
      "epoch: 731 | train_loss: 0.3777 | val_loss:2.5365 | test_loss: 2.4661\n",
      "0.5782901056676273\n",
      "epoch: 732 | train_loss: 0.3776 | val_loss:2.5697 | test_loss: 2.5045\n",
      "0.579250720461095\n",
      "epoch: 733 | train_loss: 0.3776 | val_loss:2.5535 | test_loss: 2.4761\n",
      "0.5811719500480308\n",
      "epoch: 734 | train_loss: 0.3756 | val_loss:2.5639 | test_loss: 2.5037\n",
      "0.5840537944284342\n",
      "epoch: 735 | train_loss: 0.3764 | val_loss:2.5676 | test_loss: 2.4842\n",
      "0.5821325648414986\n",
      "epoch: 736 | train_loss: 0.3750 | val_loss:2.5532 | test_loss: 2.4973\n",
      "0.5850144092219021\n",
      "epoch: 737 | train_loss: 0.3762 | val_loss:2.5910 | test_loss: 2.5040\n",
      "0.5840537944284342\n",
      "epoch: 738 | train_loss: 0.3742 | val_loss:2.5528 | test_loss: 2.4963\n",
      "0.5859750240153698\n",
      "epoch: 739 | train_loss: 0.3726 | val_loss:2.5909 | test_loss: 2.5071\n",
      "0.5811719500480308\n",
      "epoch: 740 | train_loss: 0.3691 | val_loss:2.5533 | test_loss: 2.4906\n",
      "0.5878962536023055\n",
      "epoch: 741 | train_loss: 0.3669 | val_loss:2.5812 | test_loss: 2.5062\n",
      "0.5869356388088377\n",
      "epoch: 742 | train_loss: 0.3662 | val_loss:2.5616 | test_loss: 2.4896\n",
      "0.5869356388088377\n",
      "epoch: 743 | train_loss: 0.3673 | val_loss:2.5710 | test_loss: 2.5066\n",
      "0.5802113352545629\n",
      "epoch: 744 | train_loss: 0.3698 | val_loss:2.5787 | test_loss: 2.4986\n",
      "0.5811719500480308\n",
      "epoch: 745 | train_loss: 0.3712 | val_loss:2.5754 | test_loss: 2.5178\n",
      "0.5830931796349664\n",
      "epoch: 746 | train_loss: 0.3738 | val_loss:2.5896 | test_loss: 2.5058\n",
      "0.5821325648414986\n",
      "epoch: 747 | train_loss: 0.3719 | val_loss:2.5796 | test_loss: 2.5230\n",
      "0.579250720461095\n",
      "epoch: 748 | train_loss: 0.3709 | val_loss:2.5926 | test_loss: 2.5098\n",
      "0.5840537944284342\n",
      "epoch: 749 | train_loss: 0.3669 | val_loss:2.5851 | test_loss: 2.5229\n",
      "0.5888568683957733\n",
      "epoch: 750 | train_loss: 0.3647 | val_loss:2.5820 | test_loss: 2.5063\n",
      "0.5859750240153698\n",
      "epoch: 751 | train_loss: 0.3632 | val_loss:2.5946 | test_loss: 2.5234\n",
      "0.5850144092219021\n",
      "epoch: 752 | train_loss: 0.3632 | val_loss:2.5797 | test_loss: 2.5119\n",
      "0.5888568683957733\n",
      "epoch: 753 | train_loss: 0.3637 | val_loss:2.6068 | test_loss: 2.5286\n",
      "0.5878962536023055\n",
      "epoch: 754 | train_loss: 0.3636 | val_loss:2.5801 | test_loss: 2.5168\n",
      "0.5888568683957733\n",
      "epoch: 755 | train_loss: 0.3633 | val_loss:2.6095 | test_loss: 2.5292\n",
      "0.5888568683957733\n",
      "epoch: 756 | train_loss: 0.3624 | val_loss:2.5867 | test_loss: 2.5230\n",
      "0.5888568683957733\n",
      "epoch: 757 | train_loss: 0.3615 | val_loss:2.6065 | test_loss: 2.5276\n",
      "0.5859750240153698\n",
      "epoch: 758 | train_loss: 0.3606 | val_loss:2.5961 | test_loss: 2.5291\n",
      "0.5917387127761767\n",
      "epoch: 759 | train_loss: 0.3599 | val_loss:2.6030 | test_loss: 2.5271\n",
      "0.5888568683957733\n",
      "epoch: 760 | train_loss: 0.3598 | val_loss:2.6094 | test_loss: 2.5382\n",
      "0.5888568683957733\n",
      "epoch: 761 | train_loss: 0.3599 | val_loss:2.6004 | test_loss: 2.5277\n",
      "0.5888568683957733\n",
      "epoch: 762 | train_loss: 0.3602 | val_loss:2.6197 | test_loss: 2.5458\n",
      "0.5869356388088377\n",
      "epoch: 763 | train_loss: 0.3604 | val_loss:2.6016 | test_loss: 2.5306\n",
      "0.5878962536023055\n",
      "epoch: 764 | train_loss: 0.3607 | val_loss:2.6291 | test_loss: 2.5547\n",
      "0.5840537944284342\n",
      "epoch: 765 | train_loss: 0.3614 | val_loss:2.6028 | test_loss: 2.5315\n",
      "0.5869356388088377\n",
      "epoch: 766 | train_loss: 0.3620 | val_loss:2.6351 | test_loss: 2.5623\n",
      "0.5830931796349664\n",
      "epoch: 767 | train_loss: 0.3626 | val_loss:2.6110 | test_loss: 2.5371\n",
      "0.5821325648414986\n",
      "epoch: 768 | train_loss: 0.3628 | val_loss:2.6397 | test_loss: 2.5696\n",
      "0.5830931796349664\n",
      "epoch: 769 | train_loss: 0.3631 | val_loss:2.6188 | test_loss: 2.5405\n",
      "0.579250720461095\n",
      "epoch: 770 | train_loss: 0.3621 | val_loss:2.6370 | test_loss: 2.5703\n",
      "0.5888568683957733\n",
      "epoch: 771 | train_loss: 0.3620 | val_loss:2.6325 | test_loss: 2.5493\n",
      "0.5830931796349664\n",
      "epoch: 772 | train_loss: 0.3606 | val_loss:2.6336 | test_loss: 2.5692\n",
      "0.590778097982709\n",
      "epoch: 773 | train_loss: 0.3602 | val_loss:2.6425 | test_loss: 2.5566\n",
      "0.5850144092219021\n",
      "epoch: 774 | train_loss: 0.3588 | val_loss:2.6298 | test_loss: 2.5659\n",
      "0.5898174831892411\n",
      "epoch: 775 | train_loss: 0.3580 | val_loss:2.6495 | test_loss: 2.5641\n",
      "0.5850144092219021\n",
      "epoch: 776 | train_loss: 0.3566 | val_loss:2.6296 | test_loss: 2.5639\n",
      "0.5888568683957733\n",
      "epoch: 777 | train_loss: 0.3558 | val_loss:2.6537 | test_loss: 2.5713\n",
      "0.5869356388088377\n",
      "epoch: 778 | train_loss: 0.3548 | val_loss:2.6321 | test_loss: 2.5634\n",
      "0.590778097982709\n",
      "epoch: 779 | train_loss: 0.3542 | val_loss:2.6522 | test_loss: 2.5744\n",
      "0.5888568683957733\n",
      "epoch: 780 | train_loss: 0.3537 | val_loss:2.6372 | test_loss: 2.5645\n",
      "0.5878962536023055\n",
      "epoch: 781 | train_loss: 0.3535 | val_loss:2.6527 | test_loss: 2.5798\n",
      "0.5926993275696446\n",
      "epoch: 782 | train_loss: 0.3537 | val_loss:2.6457 | test_loss: 2.5685\n",
      "0.5850144092219021\n",
      "epoch: 783 | train_loss: 0.3541 | val_loss:2.6528 | test_loss: 2.5844\n",
      "0.590778097982709\n",
      "epoch: 784 | train_loss: 0.3554 | val_loss:2.6541 | test_loss: 2.5727\n",
      "0.5850144092219021\n",
      "epoch: 785 | train_loss: 0.3563 | val_loss:2.6611 | test_loss: 2.5958\n",
      "0.5888568683957733\n",
      "epoch: 786 | train_loss: 0.3584 | val_loss:2.6650 | test_loss: 2.5797\n",
      "0.5821325648414986\n",
      "epoch: 787 | train_loss: 0.3589 | val_loss:2.6686 | test_loss: 2.6050\n",
      "0.5840537944284342\n",
      "epoch: 788 | train_loss: 0.3611 | val_loss:2.6678 | test_loss: 2.5812\n",
      "0.5811719500480308\n",
      "epoch: 789 | train_loss: 0.3589 | val_loss:2.6764 | test_loss: 2.6117\n",
      "0.5869356388088377\n",
      "epoch: 790 | train_loss: 0.3580 | val_loss:2.6707 | test_loss: 2.5858\n",
      "0.5821325648414986\n",
      "epoch: 791 | train_loss: 0.3548 | val_loss:2.6782 | test_loss: 2.6094\n",
      "0.590778097982709\n",
      "epoch: 792 | train_loss: 0.3530 | val_loss:2.6641 | test_loss: 2.5844\n",
      "0.5888568683957733\n",
      "epoch: 793 | train_loss: 0.3505 | val_loss:2.6781 | test_loss: 2.6038\n",
      "0.590778097982709\n",
      "epoch: 794 | train_loss: 0.3492 | val_loss:2.6696 | test_loss: 2.5946\n",
      "0.590778097982709\n",
      "epoch: 795 | train_loss: 0.3486 | val_loss:2.6808 | test_loss: 2.6014\n",
      "0.5869356388088377\n",
      "epoch: 796 | train_loss: 0.3489 | val_loss:2.6720 | test_loss: 2.6011\n",
      "0.5936599423631124\n",
      "epoch: 797 | train_loss: 0.3495 | val_loss:2.6850 | test_loss: 2.6016\n",
      "0.5859750240153698\n",
      "epoch: 798 | train_loss: 0.3504 | val_loss:2.6844 | test_loss: 2.6160\n",
      "0.590778097982709\n",
      "epoch: 799 | train_loss: 0.3517 | val_loss:2.6894 | test_loss: 2.6034\n",
      "0.5840537944284342\n",
      "epoch: 800 | train_loss: 0.3521 | val_loss:2.6895 | test_loss: 2.6225\n",
      "0.5936599423631124\n",
      "epoch: 801 | train_loss: 0.3531 | val_loss:2.6915 | test_loss: 2.6053\n",
      "0.5850144092219021\n",
      "epoch: 802 | train_loss: 0.3524 | val_loss:2.7012 | test_loss: 2.6328\n",
      "0.5888568683957733\n",
      "epoch: 803 | train_loss: 0.3519 | val_loss:2.6919 | test_loss: 2.6079\n",
      "0.5840537944284342\n",
      "epoch: 804 | train_loss: 0.3503 | val_loss:2.7025 | test_loss: 2.6313\n",
      "0.5926993275696446\n",
      "epoch: 805 | train_loss: 0.3489 | val_loss:2.6891 | test_loss: 2.6089\n",
      "0.5869356388088377\n",
      "epoch: 806 | train_loss: 0.3473 | val_loss:2.7087 | test_loss: 2.6328\n",
      "0.5869356388088377\n",
      "epoch: 807 | train_loss: 0.3463 | val_loss:2.6906 | test_loss: 2.6147\n",
      "0.5878962536023055\n",
      "epoch: 808 | train_loss: 0.3459 | val_loss:2.7120 | test_loss: 2.6311\n",
      "0.5869356388088377\n",
      "epoch: 809 | train_loss: 0.3457 | val_loss:2.6937 | test_loss: 2.6221\n",
      "0.5917387127761767\n",
      "epoch: 810 | train_loss: 0.3463 | val_loss:2.7227 | test_loss: 2.6366\n",
      "0.5878962536023055\n",
      "epoch: 811 | train_loss: 0.3471 | val_loss:2.6967 | test_loss: 2.6297\n",
      "0.5859750240153698\n",
      "epoch: 812 | train_loss: 0.3491 | val_loss:2.7345 | test_loss: 2.6430\n",
      "0.5869356388088377\n",
      "epoch: 813 | train_loss: 0.3505 | val_loss:2.7028 | test_loss: 2.6406\n",
      "0.5859750240153698\n",
      "epoch: 814 | train_loss: 0.3536 | val_loss:2.7512 | test_loss: 2.6546\n",
      "0.5782901056676273\n",
      "epoch: 815 | train_loss: 0.3545 | val_loss:2.7027 | test_loss: 2.6453\n",
      "0.5888568683957733\n",
      "epoch: 816 | train_loss: 0.3582 | val_loss:2.7670 | test_loss: 2.6669\n",
      "0.5840537944284342\n",
      "epoch: 817 | train_loss: 0.3575 | val_loss:2.7061 | test_loss: 2.6505\n",
      "0.5926993275696446\n",
      "epoch: 818 | train_loss: 0.3583 | val_loss:2.7761 | test_loss: 2.6773\n",
      "0.5830931796349664\n",
      "epoch: 819 | train_loss: 0.3530 | val_loss:2.7037 | test_loss: 2.6431\n",
      "0.5917387127761767\n",
      "epoch: 820 | train_loss: 0.3483 | val_loss:2.7562 | test_loss: 2.6684\n",
      "0.5840537944284342\n",
      "epoch: 821 | train_loss: 0.3450 | val_loss:2.7153 | test_loss: 2.6387\n",
      "0.5878962536023055\n",
      "epoch: 822 | train_loss: 0.3458 | val_loss:2.7359 | test_loss: 2.6651\n",
      "0.5850144092219021\n",
      "epoch: 823 | train_loss: 0.3509 | val_loss:2.7447 | test_loss: 2.6510\n",
      "0.5869356388088377\n",
      "epoch: 824 | train_loss: 0.3522 | val_loss:2.7325 | test_loss: 2.6727\n",
      "0.5830931796349664\n",
      "epoch: 825 | train_loss: 0.3535 | val_loss:2.7628 | test_loss: 2.6643\n",
      "0.5878962536023055\n",
      "epoch: 826 | train_loss: 0.3467 | val_loss:2.7347 | test_loss: 2.6701\n",
      "0.590778097982709\n",
      "epoch: 827 | train_loss: 0.3419 | val_loss:2.7418 | test_loss: 2.6546\n",
      "0.5840537944284342\n",
      "epoch: 828 | train_loss: 0.3397 | val_loss:2.7462 | test_loss: 2.6664\n",
      "0.5830931796349664\n",
      "epoch: 829 | train_loss: 0.3411 | val_loss:2.7293 | test_loss: 2.6577\n",
      "0.5936599423631124\n",
      "epoch: 830 | train_loss: 0.3437 | val_loss:2.7692 | test_loss: 2.6777\n",
      "0.5850144092219021\n",
      "epoch: 831 | train_loss: 0.3439 | val_loss:2.7307 | test_loss: 2.6655\n",
      "0.595581171950048\n",
      "epoch: 832 | train_loss: 0.3426 | val_loss:2.7692 | test_loss: 2.6779\n",
      "0.5898174831892411\n",
      "epoch: 833 | train_loss: 0.3394 | val_loss:2.7388 | test_loss: 2.6676\n",
      "0.5917387127761767\n",
      "epoch: 834 | train_loss: 0.3374 | val_loss:2.7518 | test_loss: 2.6693\n",
      "0.5936599423631124\n",
      "epoch: 835 | train_loss: 0.3374 | val_loss:2.7583 | test_loss: 2.6756\n",
      "0.5821325648414986\n",
      "epoch: 836 | train_loss: 0.3392 | val_loss:2.7403 | test_loss: 2.6681\n",
      "0.5878962536023055\n",
      "epoch: 837 | train_loss: 0.3414 | val_loss:2.7835 | test_loss: 2.6934\n",
      "0.5821325648414986\n",
      "epoch: 838 | train_loss: 0.3415 | val_loss:2.7452 | test_loss: 2.6749\n",
      "0.5898174831892411\n",
      "epoch: 839 | train_loss: 0.3404 | val_loss:2.7859 | test_loss: 2.6977\n",
      "0.5782901056676273\n",
      "epoch: 840 | train_loss: 0.3388 | val_loss:2.7496 | test_loss: 2.6723\n",
      "0.5936599423631124\n",
      "epoch: 841 | train_loss: 0.3384 | val_loss:2.7788 | test_loss: 2.6992\n",
      "0.5926993275696446\n",
      "epoch: 842 | train_loss: 0.3397 | val_loss:2.7676 | test_loss: 2.6796\n",
      "0.5869356388088377\n",
      "epoch: 843 | train_loss: 0.3411 | val_loss:2.7760 | test_loss: 2.7045\n",
      "0.5878962536023055\n",
      "epoch: 844 | train_loss: 0.3438 | val_loss:2.7778 | test_loss: 2.6843\n",
      "0.5840537944284342\n",
      "epoch: 845 | train_loss: 0.3425 | val_loss:2.7844 | test_loss: 2.7142\n",
      "0.5888568683957733\n",
      "epoch: 846 | train_loss: 0.3421 | val_loss:2.7804 | test_loss: 2.6889\n",
      "0.5869356388088377\n",
      "epoch: 847 | train_loss: 0.3393 | val_loss:2.7891 | test_loss: 2.7135\n",
      "0.5888568683957733\n",
      "epoch: 848 | train_loss: 0.3381 | val_loss:2.7698 | test_loss: 2.6866\n",
      "0.5926993275696446\n",
      "epoch: 849 | train_loss: 0.3365 | val_loss:2.7991 | test_loss: 2.7156\n",
      "0.5869356388088377\n",
      "epoch: 850 | train_loss: 0.3354 | val_loss:2.7734 | test_loss: 2.6958\n",
      "0.5946205571565802\n",
      "epoch: 851 | train_loss: 0.3344 | val_loss:2.7997 | test_loss: 2.7122\n",
      "0.5850144092219021\n",
      "epoch: 852 | train_loss: 0.3338 | val_loss:2.7746 | test_loss: 2.6988\n",
      "0.5965417867435159\n",
      "epoch: 853 | train_loss: 0.3328 | val_loss:2.7978 | test_loss: 2.7105\n",
      "0.5926993275696446\n",
      "epoch: 854 | train_loss: 0.3323 | val_loss:2.7921 | test_loss: 2.7126\n",
      "0.5936599423631124\n",
      "epoch: 855 | train_loss: 0.3322 | val_loss:2.7865 | test_loss: 2.7037\n",
      "0.5917387127761767\n",
      "epoch: 856 | train_loss: 0.3331 | val_loss:2.8051 | test_loss: 2.7214\n",
      "0.5840537944284342\n",
      "epoch: 857 | train_loss: 0.3349 | val_loss:2.7865 | test_loss: 2.7072\n",
      "0.5898174831892411\n",
      "epoch: 858 | train_loss: 0.3370 | val_loss:2.8279 | test_loss: 2.7421\n",
      "0.5802113352545629\n",
      "epoch: 859 | train_loss: 0.3398 | val_loss:2.7832 | test_loss: 2.7048\n",
      "0.5869356388088377\n",
      "epoch: 860 | train_loss: 0.3399 | val_loss:2.8309 | test_loss: 2.7477\n",
      "0.5869356388088377\n",
      "epoch: 861 | train_loss: 0.3388 | val_loss:2.7981 | test_loss: 2.7148\n",
      "0.5917387127761767\n",
      "epoch: 862 | train_loss: 0.3361 | val_loss:2.8270 | test_loss: 2.7476\n",
      "0.5965417867435159\n",
      "epoch: 863 | train_loss: 0.3346 | val_loss:2.8050 | test_loss: 2.7154\n",
      "0.5898174831892411\n",
      "epoch: 864 | train_loss: 0.3329 | val_loss:2.8042 | test_loss: 2.7313\n",
      "0.5946205571565802\n",
      "epoch: 865 | train_loss: 0.3333 | val_loss:2.8357 | test_loss: 2.7392\n",
      "0.5869356388088377\n",
      "epoch: 866 | train_loss: 0.3335 | val_loss:2.8045 | test_loss: 2.7323\n",
      "0.5965417867435159\n",
      "epoch: 867 | train_loss: 0.3336 | val_loss:2.8426 | test_loss: 2.7470\n",
      "0.5830931796349664\n",
      "epoch: 868 | train_loss: 0.3319 | val_loss:2.8063 | test_loss: 2.7293\n",
      "0.595581171950048\n",
      "epoch: 869 | train_loss: 0.3305 | val_loss:2.8408 | test_loss: 2.7523\n",
      "0.5936599423631124\n",
      "epoch: 870 | train_loss: 0.3295 | val_loss:2.8185 | test_loss: 2.7313\n",
      "0.5936599423631124\n",
      "epoch: 871 | train_loss: 0.3301 | val_loss:2.8240 | test_loss: 2.7465\n",
      "0.595581171950048\n",
      "epoch: 872 | train_loss: 0.3319 | val_loss:2.8378 | test_loss: 2.7413\n",
      "0.5898174831892411\n",
      "epoch: 873 | train_loss: 0.3329 | val_loss:2.8260 | test_loss: 2.7552\n",
      "0.5926993275696446\n",
      "epoch: 874 | train_loss: 0.3343 | val_loss:2.8427 | test_loss: 2.7441\n",
      "0.5926993275696446\n",
      "epoch: 875 | train_loss: 0.3326 | val_loss:2.8306 | test_loss: 2.7593\n",
      "0.5946205571565802\n",
      "epoch: 876 | train_loss: 0.3320 | val_loss:2.8451 | test_loss: 2.7492\n",
      "0.590778097982709\n",
      "epoch: 877 | train_loss: 0.3298 | val_loss:2.8428 | test_loss: 2.7653\n",
      "0.590778097982709\n",
      "epoch: 878 | train_loss: 0.3295 | val_loss:2.8291 | test_loss: 2.7418\n",
      "0.5936599423631124\n",
      "epoch: 879 | train_loss: 0.3288 | val_loss:2.8568 | test_loss: 2.7713\n",
      "0.5850144092219021\n",
      "epoch: 880 | train_loss: 0.3288 | val_loss:2.8332 | test_loss: 2.7517\n",
      "0.5946205571565802\n",
      "epoch: 881 | train_loss: 0.3284 | val_loss:2.8651 | test_loss: 2.7755\n",
      "0.5859750240153698\n",
      "epoch: 882 | train_loss: 0.3277 | val_loss:2.8294 | test_loss: 2.7501\n",
      "0.6013448607108549\n",
      "epoch: 883 | train_loss: 0.3259 | val_loss:2.8602 | test_loss: 2.7713\n",
      "0.5936599423631124\n",
      "epoch: 884 | train_loss: 0.3248 | val_loss:2.8438 | test_loss: 2.7610\n",
      "0.6023054755043228\n",
      "epoch: 885 | train_loss: 0.3239 | val_loss:2.8562 | test_loss: 2.7699\n",
      "0.6032660902977905\n",
      "epoch: 886 | train_loss: 0.3235 | val_loss:2.8531 | test_loss: 2.7664\n",
      "0.5994236311239193\n",
      "epoch: 887 | train_loss: 0.3234 | val_loss:2.8514 | test_loss: 2.7683\n",
      "0.6042267050912584\n",
      "epoch: 888 | train_loss: 0.3239 | val_loss:2.8700 | test_loss: 2.7798\n",
      "0.5888568683957733\n",
      "epoch: 889 | train_loss: 0.3240 | val_loss:2.8503 | test_loss: 2.7688\n",
      "0.5965417867435159\n",
      "epoch: 890 | train_loss: 0.3243 | val_loss:2.8759 | test_loss: 2.7852\n",
      "0.5859750240153698\n",
      "epoch: 891 | train_loss: 0.3246 | val_loss:2.8529 | test_loss: 2.7707\n",
      "0.5946205571565802\n",
      "epoch: 892 | train_loss: 0.3250 | val_loss:2.8848 | test_loss: 2.7962\n",
      "0.5898174831892411\n",
      "epoch: 893 | train_loss: 0.3255 | val_loss:2.8562 | test_loss: 2.7711\n",
      "0.590778097982709\n",
      "epoch: 894 | train_loss: 0.3260 | val_loss:2.8838 | test_loss: 2.8002\n",
      "0.5878962536023055\n",
      "epoch: 895 | train_loss: 0.3277 | val_loss:2.8669 | test_loss: 2.7760\n",
      "0.590778097982709\n",
      "epoch: 896 | train_loss: 0.3286 | val_loss:2.8889 | test_loss: 2.8107\n",
      "0.5917387127761767\n",
      "epoch: 897 | train_loss: 0.3312 | val_loss:2.8806 | test_loss: 2.7830\n",
      "0.5888568683957733\n",
      "epoch: 898 | train_loss: 0.3302 | val_loss:2.8864 | test_loss: 2.8130\n",
      "0.5946205571565802\n",
      "epoch: 899 | train_loss: 0.3309 | val_loss:2.8975 | test_loss: 2.7946\n",
      "0.590778097982709\n",
      "epoch: 900 | train_loss: 0.3277 | val_loss:2.8849 | test_loss: 2.8119\n",
      "0.5994236311239193\n",
      "epoch: 901 | train_loss: 0.3264 | val_loss:2.8978 | test_loss: 2.7963\n",
      "0.5926993275696446\n",
      "epoch: 902 | train_loss: 0.3230 | val_loss:2.8832 | test_loss: 2.8055\n",
      "0.5975024015369836\n",
      "epoch: 903 | train_loss: 0.3210 | val_loss:2.8982 | test_loss: 2.8026\n",
      "0.5975024015369836\n",
      "epoch: 904 | train_loss: 0.3194 | val_loss:2.8838 | test_loss: 2.7996\n",
      "0.6003842459173871\n",
      "epoch: 905 | train_loss: 0.3189 | val_loss:2.8925 | test_loss: 2.8046\n",
      "0.5984630163304515\n",
      "epoch: 906 | train_loss: 0.3193 | val_loss:2.8936 | test_loss: 2.8028\n",
      "0.5926993275696446\n",
      "epoch: 907 | train_loss: 0.3203 | val_loss:2.8966 | test_loss: 2.8152\n",
      "0.5936599423631124\n",
      "epoch: 908 | train_loss: 0.3225 | val_loss:2.8949 | test_loss: 2.8004\n",
      "0.5888568683957733\n",
      "epoch: 909 | train_loss: 0.3234 | val_loss:2.9058 | test_loss: 2.8281\n",
      "0.5917387127761767\n",
      "epoch: 910 | train_loss: 0.3258 | val_loss:2.9067 | test_loss: 2.8093\n",
      "0.5878962536023055\n",
      "epoch: 911 | train_loss: 0.3258 | val_loss:2.9161 | test_loss: 2.8388\n",
      "0.5869356388088377\n",
      "epoch: 912 | train_loss: 0.3269 | val_loss:2.9036 | test_loss: 2.8078\n",
      "0.5917387127761767\n",
      "epoch: 913 | train_loss: 0.3238 | val_loss:2.9229 | test_loss: 2.8421\n",
      "0.5917387127761767\n",
      "epoch: 914 | train_loss: 0.3219 | val_loss:2.9066 | test_loss: 2.8140\n",
      "0.5975024015369836\n",
      "epoch: 915 | train_loss: 0.3195 | val_loss:2.9230 | test_loss: 2.8371\n",
      "0.5888568683957733\n",
      "epoch: 916 | train_loss: 0.3183 | val_loss:2.9013 | test_loss: 2.8139\n",
      "0.5984630163304515\n",
      "epoch: 917 | train_loss: 0.3168 | val_loss:2.9240 | test_loss: 2.8333\n",
      "0.6003842459173871\n",
      "epoch: 918 | train_loss: 0.3163 | val_loss:2.9125 | test_loss: 2.8275\n",
      "0.6013448607108549\n",
      "epoch: 919 | train_loss: 0.3159 | val_loss:2.9233 | test_loss: 2.8298\n",
      "0.5975024015369836\n",
      "epoch: 920 | train_loss: 0.3163 | val_loss:2.9168 | test_loss: 2.8338\n",
      "0.595581171950048\n",
      "epoch: 921 | train_loss: 0.3168 | val_loss:2.9271 | test_loss: 2.8317\n",
      "0.5946205571565802\n",
      "epoch: 922 | train_loss: 0.3174 | val_loss:2.9305 | test_loss: 2.8482\n",
      "0.5936599423631124\n",
      "epoch: 923 | train_loss: 0.3185 | val_loss:2.9249 | test_loss: 2.8296\n",
      "0.5917387127761767\n",
      "epoch: 924 | train_loss: 0.3188 | val_loss:2.9385 | test_loss: 2.8562\n",
      "0.5878962536023055\n",
      "epoch: 925 | train_loss: 0.3198 | val_loss:2.9289 | test_loss: 2.8346\n",
      "0.5917387127761767\n",
      "epoch: 926 | train_loss: 0.3201 | val_loss:2.9508 | test_loss: 2.8668\n",
      "0.5859750240153698\n",
      "epoch: 927 | train_loss: 0.3212 | val_loss:2.9227 | test_loss: 2.8322\n",
      "0.5946205571565802\n",
      "epoch: 928 | train_loss: 0.3197 | val_loss:2.9557 | test_loss: 2.8692\n",
      "0.5859750240153698\n",
      "epoch: 929 | train_loss: 0.3181 | val_loss:2.9289 | test_loss: 2.8410\n",
      "0.5984630163304515\n",
      "epoch: 930 | train_loss: 0.3160 | val_loss:2.9569 | test_loss: 2.8669\n",
      "0.5898174831892411\n",
      "epoch: 931 | train_loss: 0.3142 | val_loss:2.9288 | test_loss: 2.8429\n",
      "0.6003842459173871\n",
      "epoch: 932 | train_loss: 0.3128 | val_loss:2.9514 | test_loss: 2.8588\n",
      "0.5975024015369836\n",
      "epoch: 933 | train_loss: 0.3125 | val_loss:2.9405 | test_loss: 2.8554\n",
      "0.5984630163304515\n",
      "epoch: 934 | train_loss: 0.3126 | val_loss:2.9552 | test_loss: 2.8597\n",
      "0.5946205571565802\n",
      "epoch: 935 | train_loss: 0.3129 | val_loss:2.9457 | test_loss: 2.8628\n",
      "0.5994236311239193\n",
      "epoch: 936 | train_loss: 0.3134 | val_loss:2.9600 | test_loss: 2.8618\n",
      "0.5965417867435159\n",
      "epoch: 937 | train_loss: 0.3139 | val_loss:2.9542 | test_loss: 2.8734\n",
      "0.5975024015369836\n",
      "epoch: 938 | train_loss: 0.3152 | val_loss:2.9610 | test_loss: 2.8616\n",
      "0.5946205571565802\n",
      "epoch: 939 | train_loss: 0.3157 | val_loss:2.9621 | test_loss: 2.8828\n",
      "0.590778097982709\n",
      "epoch: 940 | train_loss: 0.3175 | val_loss:2.9674 | test_loss: 2.8668\n",
      "0.5917387127761767\n",
      "epoch: 941 | train_loss: 0.3174 | val_loss:2.9709 | test_loss: 2.8922\n",
      "0.5898174831892411\n",
      "epoch: 942 | train_loss: 0.3190 | val_loss:2.9649 | test_loss: 2.8653\n",
      "0.590778097982709\n",
      "epoch: 943 | train_loss: 0.3163 | val_loss:2.9748 | test_loss: 2.8949\n",
      "0.5917387127761767\n",
      "epoch: 944 | train_loss: 0.3152 | val_loss:2.9707 | test_loss: 2.8725\n",
      "0.5984630163304515\n",
      "epoch: 945 | train_loss: 0.3127 | val_loss:2.9761 | test_loss: 2.8928\n",
      "0.595581171950048\n",
      "epoch: 946 | train_loss: 0.3113 | val_loss:2.9677 | test_loss: 2.8732\n",
      "0.5984630163304515\n",
      "epoch: 947 | train_loss: 0.3097 | val_loss:2.9767 | test_loss: 2.8895\n",
      "0.5965417867435159\n",
      "epoch: 948 | train_loss: 0.3088 | val_loss:2.9748 | test_loss: 2.8831\n",
      "0.5965417867435159\n",
      "epoch: 949 | train_loss: 0.3082 | val_loss:2.9773 | test_loss: 2.8868\n",
      "0.5965417867435159\n",
      "epoch: 950 | train_loss: 0.3081 | val_loss:2.9761 | test_loss: 2.8875\n",
      "0.5994236311239193\n",
      "epoch: 951 | train_loss: 0.3081 | val_loss:2.9828 | test_loss: 2.8894\n",
      "0.595581171950048\n",
      "epoch: 952 | train_loss: 0.3083 | val_loss:2.9826 | test_loss: 2.8966\n",
      "0.595581171950048\n",
      "epoch: 953 | train_loss: 0.3092 | val_loss:2.9811 | test_loss: 2.8864\n",
      "0.5984630163304515\n",
      "epoch: 954 | train_loss: 0.3097 | val_loss:2.9917 | test_loss: 2.9076\n",
      "0.5936599423631124\n",
      "epoch: 955 | train_loss: 0.3113 | val_loss:2.9885 | test_loss: 2.8921\n",
      "0.5946205571565802\n",
      "epoch: 956 | train_loss: 0.3126 | val_loss:3.0032 | test_loss: 2.9200\n",
      "0.5859750240153698\n",
      "epoch: 957 | train_loss: 0.3151 | val_loss:2.9871 | test_loss: 2.8911\n",
      "0.5946205571565802\n",
      "epoch: 958 | train_loss: 0.3140 | val_loss:3.0129 | test_loss: 2.9291\n",
      "0.5878962536023055\n",
      "epoch: 959 | train_loss: 0.3138 | val_loss:2.9929 | test_loss: 2.8975\n",
      "0.5946205571565802\n",
      "epoch: 960 | train_loss: 0.3112 | val_loss:3.0140 | test_loss: 2.9279\n",
      "0.590778097982709\n",
      "epoch: 961 | train_loss: 0.3099 | val_loss:2.9905 | test_loss: 2.8982\n",
      "0.5994236311239193\n",
      "epoch: 962 | train_loss: 0.3071 | val_loss:3.0107 | test_loss: 2.9213\n",
      "0.5975024015369836\n",
      "epoch: 963 | train_loss: 0.3056 | val_loss:3.0035 | test_loss: 2.9124\n",
      "0.5975024015369836\n",
      "epoch: 964 | train_loss: 0.3048 | val_loss:3.0091 | test_loss: 2.9164\n",
      "0.5984630163304515\n",
      "epoch: 965 | train_loss: 0.3051 | val_loss:3.0090 | test_loss: 2.9200\n",
      "0.595581171950048\n",
      "epoch: 966 | train_loss: 0.3058 | val_loss:3.0105 | test_loss: 2.9153\n",
      "0.5984630163304515\n",
      "epoch: 967 | train_loss: 0.3072 | val_loss:3.0251 | test_loss: 2.9377\n",
      "0.5926993275696446\n",
      "epoch: 968 | train_loss: 0.3091 | val_loss:3.0101 | test_loss: 2.9135\n",
      "0.595581171950048\n",
      "epoch: 969 | train_loss: 0.3105 | val_loss:3.0330 | test_loss: 2.9476\n",
      "0.5869356388088377\n",
      "epoch: 970 | train_loss: 0.3122 | val_loss:3.0155 | test_loss: 2.9186\n",
      "0.5965417867435159\n",
      "epoch: 971 | train_loss: 0.3117 | val_loss:3.0413 | test_loss: 2.9559\n",
      "0.5850144092219021\n",
      "epoch: 972 | train_loss: 0.3113 | val_loss:3.0115 | test_loss: 2.9169\n",
      "0.5994236311239193\n",
      "epoch: 973 | train_loss: 0.3079 | val_loss:3.0373 | test_loss: 2.9499\n",
      "0.5975024015369836\n",
      "epoch: 974 | train_loss: 0.3052 | val_loss:3.0191 | test_loss: 2.9264\n",
      "0.5975024015369836\n",
      "epoch: 975 | train_loss: 0.3032 | val_loss:3.0339 | test_loss: 2.9424\n",
      "0.5946205571565802\n",
      "epoch: 976 | train_loss: 0.3026 | val_loss:3.0202 | test_loss: 2.9312\n",
      "0.5984630163304515\n",
      "epoch: 977 | train_loss: 0.3024 | val_loss:3.0384 | test_loss: 2.9416\n",
      "0.5965417867435159\n",
      "epoch: 978 | train_loss: 0.3033 | val_loss:3.0301 | test_loss: 2.9455\n",
      "0.5994236311239193\n",
      "epoch: 979 | train_loss: 0.3052 | val_loss:3.0508 | test_loss: 2.9470\n",
      "0.595581171950048\n",
      "epoch: 980 | train_loss: 0.3079 | val_loss:3.0304 | test_loss: 2.9541\n",
      "0.5926993275696446\n",
      "epoch: 981 | train_loss: 0.3151 | val_loss:3.0827 | test_loss: 2.9686\n",
      "0.5878962536023055\n",
      "epoch: 982 | train_loss: 0.3223 | val_loss:3.0331 | test_loss: 2.9702\n",
      "0.5811719500480308\n",
      "epoch: 983 | train_loss: 0.3468 | val_loss:3.1494 | test_loss: 3.0190\n",
      "0.5600384245917387\n",
      "epoch: 984 | train_loss: 0.3666 | val_loss:3.0581 | test_loss: 3.0146\n",
      "0.5725264169068204\n",
      "epoch: 985 | train_loss: 0.4224 | val_loss:3.2899 | test_loss: 3.1432\n",
      "0.5600384245917387\n",
      "epoch: 986 | train_loss: 0.3841 | val_loss:3.0801 | test_loss: 3.0361\n",
      "0.590778097982709\n",
      "epoch: 987 | train_loss: 0.3285 | val_loss:3.1348 | test_loss: 3.0202\n",
      "0.590778097982709\n",
      "epoch: 988 | train_loss: 0.3160 | val_loss:3.0587 | test_loss: 2.9501\n",
      "0.5677233429394812\n",
      "epoch: 989 | train_loss: 0.3601 | val_loss:3.0481 | test_loss: 3.0041\n",
      "0.5840537944284342\n",
      "epoch: 990 | train_loss: 0.3852 | val_loss:3.2303 | test_loss: 3.0916\n",
      "0.5965417867435159\n",
      "epoch: 991 | train_loss: 0.3025 | val_loss:3.0439 | test_loss: 2.9586\n",
      "0.5706051873198847\n",
      "epoch: 992 | train_loss: 0.3674 | val_loss:3.0253 | test_loss: 2.9884\n",
      "0.5677233429394812\n",
      "epoch: 993 | train_loss: 0.4296 | val_loss:3.2825 | test_loss: 3.1322\n",
      "0.5600384245917387\n",
      "epoch: 994 | train_loss: 0.3636 | val_loss:3.1372 | test_loss: 3.0770\n",
      "0.5686839577329491\n",
      "epoch: 995 | train_loss: 0.3690 | val_loss:3.0545 | test_loss: 3.0177\n",
      "0.5504322766570605\n",
      "epoch: 996 | train_loss: 0.5110 | val_loss:3.3685 | test_loss: 3.2121\n",
      "0.5648414985590778\n",
      "epoch: 997 | train_loss: 0.3561 | val_loss:3.0738 | test_loss: 3.0261\n",
      "0.5571565802113353\n",
      "epoch: 998 | train_loss: 0.3949 | val_loss:3.1759 | test_loss: 3.1292\n",
      "0.5763688760806917\n",
      "epoch: 999 | train_loss: 0.4104 | val_loss:3.2362 | test_loss: 3.0854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(200):\n",
    "    gcn.train()\n",
    "    logits = gcn(g, g.node_feat['feat'])\n",
    "    loss = criterion(logits[dataset.train_index], y[dataset.train_index])\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.clear_grad()\n",
    "    gcn.eval()\n",
    "    loss2 = criterion(logits[dataset.val_index], y[dataset.val_index])\n",
    "    # 30 epoch 不下降 break\n",
    "    # 《30,200》  《200,1000》  <early_stop , num_epochs>\n",
    "    print(\"epoch: %s | train_loss: %.4f | val_loss:%.4f\" % (epoch, loss.numpy(), loss2.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = gcn(g, g.node_feat['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5619596541786743\n"
     ]
    }
   ],
   "source": [
    "m = paddle.metric.Accuracy()\n",
    "correct = m.compute(logits[dataset.test_index], y[dataset.test_index])\n",
    "m.update(correct)\n",
    "res = m.accumulate()# 计算 top-k（topk 中的最大值）的索引\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[4, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = paddle.to_tensor(np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.1, 0.4, 0.3, 0.2],\n",
    "    [0.1, 0.2, 0.4, 0.3],\n",
    "    [0.1, 0.2, 0.3, 0.4]]))\n",
    "y = paddle.to_tensor(np.array([[0], [1], [2], [3]]))\n",
    "m = paddle.metric.Accuracy()\n",
    "correct = m.compute(x, y)\n",
    "m.update(correct)\n",
    "res = m.accumulate()\n",
    "print(correct) # 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1041, 1], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0],\n",
      "        [3],\n",
      "        [0],\n",
      "        ...,\n",
      "        [1],\n",
      "        [0],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "print(y[dataset.test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b642dd635b0ef59147d30a45f1f67a9632d6684c8b670e646cd8f95d921a4e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}